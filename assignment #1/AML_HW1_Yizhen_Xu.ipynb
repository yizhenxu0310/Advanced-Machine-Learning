{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Github repo link\n",
    "# https://github.com/yizhenxu0310/HW1_AML_YizhenXu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y_3SmrbKHhmt"
   },
   "source": [
    "# Data Preprocess via Excel to include column 'region'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "colab_type": "code",
    "id": "vHjX9bYWH8FB",
    "outputId": "03cbaebf-fa3e-4e22-e5df-618d52f1d5fe"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Happiness_level</th>\n",
       "      <th>Country or region</th>\n",
       "      <th>region</th>\n",
       "      <th>GDP per capita</th>\n",
       "      <th>Social support</th>\n",
       "      <th>Healthy life expectancy</th>\n",
       "      <th>Freedom to make life choices</th>\n",
       "      <th>Generosity</th>\n",
       "      <th>Perceptions of corruption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Very High</td>\n",
       "      <td>Finland</td>\n",
       "      <td>Europe</td>\n",
       "      <td>1.340</td>\n",
       "      <td>1.587</td>\n",
       "      <td>0.986</td>\n",
       "      <td>0.596</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Very High</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>Europe</td>\n",
       "      <td>1.383</td>\n",
       "      <td>1.573</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Very High</td>\n",
       "      <td>Norway</td>\n",
       "      <td>Europe</td>\n",
       "      <td>1.488</td>\n",
       "      <td>1.582</td>\n",
       "      <td>1.028</td>\n",
       "      <td>0.603</td>\n",
       "      <td>0.271</td>\n",
       "      <td>0.341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Very High</td>\n",
       "      <td>Iceland</td>\n",
       "      <td>Europe</td>\n",
       "      <td>1.380</td>\n",
       "      <td>1.624</td>\n",
       "      <td>1.026</td>\n",
       "      <td>0.591</td>\n",
       "      <td>0.354</td>\n",
       "      <td>0.118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Very High</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>Europe</td>\n",
       "      <td>1.396</td>\n",
       "      <td>1.522</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.557</td>\n",
       "      <td>0.322</td>\n",
       "      <td>0.298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>Very Low</td>\n",
       "      <td>Rwanda</td>\n",
       "      <td>Africa</td>\n",
       "      <td>0.359</td>\n",
       "      <td>0.711</td>\n",
       "      <td>0.614</td>\n",
       "      <td>0.555</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>Very Low</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Asia</td>\n",
       "      <td>0.476</td>\n",
       "      <td>0.885</td>\n",
       "      <td>0.499</td>\n",
       "      <td>0.417</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>Very Low</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Asia</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.517</td>\n",
       "      <td>0.361</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>Very Low</td>\n",
       "      <td>Central African Republic</td>\n",
       "      <td>Africa</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>Very Low</td>\n",
       "      <td>South Sudan</td>\n",
       "      <td>Africa</td>\n",
       "      <td>0.306</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.295</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>156 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Happiness_level  ... Perceptions of corruption\n",
       "0         Very High  ...                     0.393\n",
       "1         Very High  ...                     0.410\n",
       "2         Very High  ...                     0.341\n",
       "3         Very High  ...                     0.118\n",
       "4         Very High  ...                     0.298\n",
       "..              ...  ...                       ...\n",
       "151        Very Low  ...                     0.411\n",
       "152        Very Low  ...                     0.147\n",
       "153        Very Low  ...                     0.025\n",
       "154        Very Low  ...                     0.035\n",
       "155        Very Low  ...                     0.091\n",
       "\n",
       "[156 rows x 9 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= pd.read_csv('worldhappiness2019_new.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EkyCaA4dHfPx"
   },
   "source": [
    "# Explore bivariate results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IWYnDkD1IVze"
   },
   "source": [
    "### The relationship between Happiness_level and GDP per capita is that the higher GDP per capita, the higher happiness level.\n",
    "\n",
    "### The relationship between Happiness_level and Social support is that the higher social support, the higher happiness level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "colab_type": "code",
    "id": "JXrc71AWCF_q",
    "outputId": "cee73c13-965a-42d5-9279-f48fd94d855d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fd89f9a1fd0>"
      ]
     },
     "execution_count": 46,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEJCAYAAAC61nFHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,axes=plt.subplots(1,2,sharey=True)\n",
    "sns.boxplot(x=\"Happiness_level\",y=\"GDP per capita\",data=df,ax=axes[0])\n",
    "sns.boxplot(x=\"Happiness_level\",y=\"Social support\",data=df,ax=axes[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rhduvVDbLEzf"
   },
   "source": [
    "### The relationship between Happiness_level and Freedom to make life choices is that the higher level of freedom, the higher happiness level.\n",
    "### The relationship between Happiness_level and Healthy life expectancy is that the longer healthy life expectancy, the higher happiness level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "colab_type": "code",
    "id": "TUbTWaLhG32y",
    "outputId": "c3664354-6077-46bc-8955-9b35aa7e121d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fd89f880e48>"
      ]
     },
     "execution_count": 47,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAEHCAYAAABbZ7oVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,axes=plt.subplots(1,2,sharey=True)\n",
    "sns.boxplot(x=\"Happiness_level\",y=\"Freedom to make life choices\",data=df,ax=axes[0])\n",
    "sns.boxplot(x=\"Happiness_level\",y=\"Healthy life expectancy\",data=df,ax=axes[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nXEPmpj7Lc2I"
   },
   "source": [
    "### The relationship between Happiness_level and Generosity is that when happiness level is very high, the generosiy level is the highest. However, when happiness level is high, the generosity level is the lowest. However, when the happiness level goes from hihg to very low, the generosity level decreases. \n",
    "### The relationship between Happiness_level and Perceptions of corruption is that when the happiness level is very high, the perception of corruption is highest. And when the happiness level is average, the perception of corruption is lowest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "colab_type": "code",
    "id": "qBvv97kAHMP1",
    "outputId": "7a83f880-8d3b-488c-8989-bdc65f35d485"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fd89f6e3860>"
      ]
     },
     "execution_count": 48,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAEHCAYAAABbZ7oVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJGFmZjU5SZiZWU3/C3sYS0ZzvuIjAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,axes=plt.subplots(1,2,sharey=True)\n",
    "sns.boxplot(x=\"Happiness_level\",y=\"Generosity\",data=df,ax=axes[0])\n",
    "sns.boxplot(x=\"Happiness_level\",y=\"Perceptions of corruption\",data=df,ax=axes[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFFBcYtYSH1C"
   },
   "source": [
    "### The relationship between Happiness_level and region is that Europe has the most above average happiness level, Asia and Americas has the same, and Africa has the least.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "VOZvY7tjNgoB",
    "outputId": "741e8b04-90b7-435b-d18a-45e1b6541017"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Happiness_level</th>\n",
       "      <th>Country or region</th>\n",
       "      <th>region</th>\n",
       "      <th>GDP per capita</th>\n",
       "      <th>Social support</th>\n",
       "      <th>Healthy life expectancy</th>\n",
       "      <th>Freedom to make life choices</th>\n",
       "      <th>Generosity</th>\n",
       "      <th>Perceptions of corruption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Very High</td>\n",
       "      <td>Finland</td>\n",
       "      <td>Europe</td>\n",
       "      <td>1.340</td>\n",
       "      <td>1.587</td>\n",
       "      <td>0.986</td>\n",
       "      <td>0.596</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Very High</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>Europe</td>\n",
       "      <td>1.383</td>\n",
       "      <td>1.573</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Very High</td>\n",
       "      <td>Norway</td>\n",
       "      <td>Europe</td>\n",
       "      <td>1.488</td>\n",
       "      <td>1.582</td>\n",
       "      <td>1.028</td>\n",
       "      <td>0.603</td>\n",
       "      <td>0.271</td>\n",
       "      <td>0.341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Very High</td>\n",
       "      <td>Iceland</td>\n",
       "      <td>Europe</td>\n",
       "      <td>1.380</td>\n",
       "      <td>1.624</td>\n",
       "      <td>1.026</td>\n",
       "      <td>0.591</td>\n",
       "      <td>0.354</td>\n",
       "      <td>0.118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Very High</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>Europe</td>\n",
       "      <td>1.396</td>\n",
       "      <td>1.522</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.557</td>\n",
       "      <td>0.322</td>\n",
       "      <td>0.298</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Happiness_level Country or region  ... Generosity  Perceptions of corruption\n",
       "0       Very High           Finland  ...      0.153                      0.393\n",
       "1       Very High           Denmark  ...      0.252                      0.410\n",
       "2       Very High            Norway  ...      0.271                      0.341\n",
       "3       Very High           Iceland  ...      0.354                      0.118\n",
       "4       Very High       Netherlands  ...      0.322                      0.298\n",
       "\n",
       "[5 rows x 9 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1=df[(df['Happiness_level']=='Very High')]\n",
    "df2=df[(df['Happiness_level']=='High')]\n",
    "#to choose those whose hapiness level is above average, including very high and high\n",
    "dfnew=df1.append(df2)   \n",
    "dfnew.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "id": "J66zPmNjQ4sR",
    "outputId": "4a797792-c684-4b7c-f870-14e525fd3bfb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Europe      25\n",
       "Asia        17\n",
       "Americas    17\n",
       "Oceania      3\n",
       "Africa       1\n",
       "Name: region, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to count the number of each region\n",
    "dfnew['region'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dKYu7vTTSGmF"
   },
   "source": [
    "# feature selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "colab_type": "code",
    "id": "WFnXfOSLTUu6",
    "outputId": "106091c6-cf6f-465c-cb68-0cf5be47539a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Happiness_level</th>\n",
       "      <th>Country or region</th>\n",
       "      <th>region</th>\n",
       "      <th>GDP per capita</th>\n",
       "      <th>Social support</th>\n",
       "      <th>Healthy life expectancy</th>\n",
       "      <th>Freedom to make life choices</th>\n",
       "      <th>Generosity</th>\n",
       "      <th>Perceptions of corruption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Very High</td>\n",
       "      <td>Finland</td>\n",
       "      <td>3</td>\n",
       "      <td>1.340</td>\n",
       "      <td>1.587</td>\n",
       "      <td>0.986</td>\n",
       "      <td>0.596</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Very High</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>3</td>\n",
       "      <td>1.383</td>\n",
       "      <td>1.573</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Very High</td>\n",
       "      <td>Norway</td>\n",
       "      <td>3</td>\n",
       "      <td>1.488</td>\n",
       "      <td>1.582</td>\n",
       "      <td>1.028</td>\n",
       "      <td>0.603</td>\n",
       "      <td>0.271</td>\n",
       "      <td>0.341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Very High</td>\n",
       "      <td>Iceland</td>\n",
       "      <td>3</td>\n",
       "      <td>1.380</td>\n",
       "      <td>1.624</td>\n",
       "      <td>1.026</td>\n",
       "      <td>0.591</td>\n",
       "      <td>0.354</td>\n",
       "      <td>0.118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Very High</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>3</td>\n",
       "      <td>1.396</td>\n",
       "      <td>1.522</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.557</td>\n",
       "      <td>0.322</td>\n",
       "      <td>0.298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>Very Low</td>\n",
       "      <td>Rwanda</td>\n",
       "      <td>0</td>\n",
       "      <td>0.359</td>\n",
       "      <td>0.711</td>\n",
       "      <td>0.614</td>\n",
       "      <td>0.555</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>Very Low</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>2</td>\n",
       "      <td>0.476</td>\n",
       "      <td>0.885</td>\n",
       "      <td>0.499</td>\n",
       "      <td>0.417</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>Very Low</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.517</td>\n",
       "      <td>0.361</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>Very Low</td>\n",
       "      <td>Central African Republic</td>\n",
       "      <td>0</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>Very Low</td>\n",
       "      <td>South Sudan</td>\n",
       "      <td>0</td>\n",
       "      <td>0.306</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.295</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>156 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Happiness_level  ... Perceptions of corruption\n",
       "0         Very High  ...                     0.393\n",
       "1         Very High  ...                     0.410\n",
       "2         Very High  ...                     0.341\n",
       "3         Very High  ...                     0.118\n",
       "4         Very High  ...                     0.298\n",
       "..              ...  ...                       ...\n",
       "151        Very Low  ...                     0.411\n",
       "152        Very Low  ...                     0.147\n",
       "153        Very Low  ...                     0.025\n",
       "154        Very Low  ...                     0.035\n",
       "155        Very Low  ...                     0.091\n",
       "\n",
       "[156 rows x 9 columns]"
      ]
     },
     "execution_count": 154,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert 'region' type\n",
    "from sklearn import preprocessing\n",
    "def convert(data):\n",
    "    number=preprocessing.LabelEncoder()\n",
    "    data['region']=number.fit_transform(data['region'])\n",
    "    return data\n",
    "\n",
    "convert(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "9zSr45myTnkU",
    "outputId": "08844f83-4330-4810-a3a6-b9df63dd10c6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region</th>\n",
       "      <th>GDP per capita</th>\n",
       "      <th>Social support</th>\n",
       "      <th>Healthy life expectancy</th>\n",
       "      <th>Freedom to make life choices</th>\n",
       "      <th>Generosity</th>\n",
       "      <th>Perceptions of corruption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1.340</td>\n",
       "      <td>1.587</td>\n",
       "      <td>0.986</td>\n",
       "      <td>0.596</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1.383</td>\n",
       "      <td>1.573</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1.488</td>\n",
       "      <td>1.582</td>\n",
       "      <td>1.028</td>\n",
       "      <td>0.603</td>\n",
       "      <td>0.271</td>\n",
       "      <td>0.341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.380</td>\n",
       "      <td>1.624</td>\n",
       "      <td>1.026</td>\n",
       "      <td>0.591</td>\n",
       "      <td>0.354</td>\n",
       "      <td>0.118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1.396</td>\n",
       "      <td>1.522</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.557</td>\n",
       "      <td>0.322</td>\n",
       "      <td>0.298</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   region  GDP per capita  ...  Generosity  Perceptions of corruption\n",
       "0       3           1.340  ...       0.153                      0.393\n",
       "1       3           1.383  ...       0.252                      0.410\n",
       "2       3           1.488  ...       0.271                      0.341\n",
       "3       3           1.380  ...       0.354                      0.118\n",
       "4       3           1.396  ...       0.322                      0.298\n",
       "\n",
       "[5 rows x 7 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=df['Happiness_level']\n",
    "X=df.drop(['Happiness_level'],axis=1)\n",
    "X=X.drop(['Country or region'],axis=1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "id": "VVP5J_qqTawB",
    "outputId": "3020dfd7-cf2b-40e6-b898-7553b488c7cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Feature      Score\n",
      "0                     region  37.512206\n",
      "1             GDP per capita  17.212230\n",
      "3    Healthy life expectancy   7.997161\n",
      "2             Social support   6.465524\n",
      "6  Perceptions of corruption   3.387830\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "#apply SelectKBest class to extract top 5 best features\n",
    "bestfeatures = SelectKBest(score_func=chi2, k=5)\n",
    "fit = bestfeatures.fit(X,y)\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(X.columns)\n",
    "#concat two dataframes for better visualization \n",
    "featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "featureScores.columns = ['Feature','Score']  #naming the dataframe columns\n",
    "print(featureScores.nlargest(5,'Score'))  #print 5 best features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Caq05m9eThsh"
   },
   "source": [
    "I use SelectKBest to do feature selection and to choose 5 top variables among all 7 variables. From the result we can see that region is the most important variable related with happiness level. The second important variable is GPD per capital. And the following three variables are healthy life expectancy, social support and perceptions of corruption, sequentially. But there are no big differences among the latter three variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LrbxFvRwV1jr"
   },
   "source": [
    "# Three prediction models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cpFvbGnnXE9u"
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv('worldhappiness2019_new.csv')\n",
    "y=data['Happiness_level']\n",
    "X=data.drop(['Happiness_level'],axis=1)\n",
    "X=X.drop(['Country or region'],axis=1)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "krpF9znhITSd"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "# We create the preprocessing pipelines for both numeric and categorical data.\n",
    "\n",
    "numeric_features=X.columns.tolist()\n",
    "numeric_features.remove('region')\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_features = ['region']\n",
    "\n",
    "#Replacing missing values with Modal value and then one hot encoding.\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# final preprocessor object set up with ColumnTransformer\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "#Fit your preprocessor object\n",
    "input_preprocessor=preprocessor.fit(X_train)\n",
    "\n",
    "import pickle\n",
    "pickle.dump(input_preprocessor, open( \"preprocessor.pkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "0HfbTIQsIb4r",
    "outputId": "adefe380-4ada-4e88-c869-928071b0fde3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(117, 11)"
      ]
     },
     "execution_count": 156,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check shape for keras input:\n",
    "input_preprocessor.transform(X_train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "WM0ULEJCIXRW",
    "outputId": "f0bddd6d-8d9c-4538-8da2-1d1752075684"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(117, 5)"
      ]
     },
     "execution_count": 157,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.get_dummies(y_train).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OhDHCmIpfIW_"
   },
   "source": [
    "### Model 1 -- decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "ShJzbvB_gdCh",
    "outputId": "4ceb6e0f-8a83-4027-9d69-8a464ebd779b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters: {'max_depth': 10}\n",
      "test-set score: 0.436\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "param_grid = {'max_depth': np.arange(1,25,1)}\n",
    "grid = GridSearchCV(DecisionTreeClassifier(random_state=0),param_grid=param_grid, cv=kfold)\n",
    "grid.fit(input_preprocessor.transform(X_train), pd.get_dummies(y_train))\n",
    "\n",
    "print(\"best parameters: {}\".format(grid.best_params_))\n",
    "print(\"test-set score: {:.3f}\".format(grid.score(input_preprocessor.transform(X_test), pd.get_dummies(y_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GOYeOBR9jsaP"
   },
   "source": [
    "### When using decision tree, the best parameter is max_depth of 10. ANd the prediction accuracy rate is 0.436.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eDFE36nJgzF7"
   },
   "source": [
    "### Model 2 -- KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dw31IfT3g2UH"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(n_splits=5,shuffle=True)    \n",
    "skfold = StratifiedKFold(n_splits=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "ItiInVYEg4ep",
    "outputId": "2dfbc970-a7e7-42b7-a9b4-0bfbbb1ff176"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters: {'n_neighbors': 1}\n",
      "test-set score: 0.564\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "param_grid = {'n_neighbors': np.arange(1, 15, 2)}\n",
    "grid = GridSearchCV(KNeighborsClassifier(),param_grid=param_grid,cv=kfold)#set the cross validation methods to kfold for further use\n",
    "model=grid.fit(input_preprocessor.transform(X_train), pd.get_dummies(y_train))\n",
    "print(\"best parameters: {}\".format(grid.best_params_))\n",
    "print(\"test-set score: {:.3f}\".format(grid.score(input_preprocessor.transform(X_test), pd.get_dummies(y_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9pYni7BHj-DV"
   },
   "source": [
    "### When using knn, the best parameter is n_neighbors of 1. ANd the prediction accuracy rate is 0.564."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uxn1rAuIchi-"
   },
   "source": [
    "### Model 3 -- Nueral Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "U5rkk1GsIhGF",
    "outputId": "840bca95-c6d5-4b91-ee8a-57de002abade"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "117/117 [==============================] - 10s 87ms/step - loss: 1.5983 - acc: 0.2393\n",
      "Epoch 2/500\n",
      "117/117 [==============================] - 0s 124us/step - loss: 1.5909 - acc: 0.2735\n",
      "Epoch 3/500\n",
      "117/117 [==============================] - 0s 114us/step - loss: 1.5841 - acc: 0.2650\n",
      "Epoch 4/500\n",
      "117/117 [==============================] - 0s 82us/step - loss: 1.5767 - acc: 0.2735\n",
      "Epoch 5/500\n",
      "117/117 [==============================] - 0s 123us/step - loss: 1.5696 - acc: 0.2991\n",
      "Epoch 6/500\n",
      "117/117 [==============================] - 0s 122us/step - loss: 1.5627 - acc: 0.3077\n",
      "Epoch 7/500\n",
      "117/117 [==============================] - 0s 110us/step - loss: 1.5558 - acc: 0.3333\n",
      "Epoch 8/500\n",
      "117/117 [==============================] - 0s 126us/step - loss: 1.5491 - acc: 0.3419\n",
      "Epoch 9/500\n",
      "117/117 [==============================] - 0s 126us/step - loss: 1.5420 - acc: 0.3590\n",
      "Epoch 10/500\n",
      "117/117 [==============================] - 0s 98us/step - loss: 1.5351 - acc: 0.3590\n",
      "Epoch 11/500\n",
      "117/117 [==============================] - 0s 122us/step - loss: 1.5284 - acc: 0.3761\n",
      "Epoch 12/500\n",
      "117/117 [==============================] - 0s 119us/step - loss: 1.5218 - acc: 0.3846\n",
      "Epoch 13/500\n",
      "117/117 [==============================] - 0s 122us/step - loss: 1.5151 - acc: 0.4017\n",
      "Epoch 14/500\n",
      "117/117 [==============================] - 0s 101us/step - loss: 1.5085 - acc: 0.4359\n",
      "Epoch 15/500\n",
      "117/117 [==============================] - 0s 104us/step - loss: 1.5021 - acc: 0.4444\n",
      "Epoch 16/500\n",
      "117/117 [==============================] - 0s 111us/step - loss: 1.4959 - acc: 0.4530\n",
      "Epoch 17/500\n",
      "117/117 [==============================] - 0s 115us/step - loss: 1.4897 - acc: 0.4615\n",
      "Epoch 18/500\n",
      "117/117 [==============================] - 0s 103us/step - loss: 1.4838 - acc: 0.4615\n",
      "Epoch 19/500\n",
      "117/117 [==============================] - 0s 121us/step - loss: 1.4779 - acc: 0.4701\n",
      "Epoch 20/500\n",
      "117/117 [==============================] - 0s 112us/step - loss: 1.4720 - acc: 0.4957\n",
      "Epoch 21/500\n",
      "117/117 [==============================] - 0s 122us/step - loss: 1.4662 - acc: 0.5043\n",
      "Epoch 22/500\n",
      "117/117 [==============================] - 0s 112us/step - loss: 1.4607 - acc: 0.5043\n",
      "Epoch 23/500\n",
      "117/117 [==============================] - 0s 112us/step - loss: 1.4548 - acc: 0.5043\n",
      "Epoch 24/500\n",
      "117/117 [==============================] - 0s 88us/step - loss: 1.4495 - acc: 0.5043\n",
      "Epoch 25/500\n",
      "117/117 [==============================] - 0s 130us/step - loss: 1.4438 - acc: 0.4957\n",
      "Epoch 26/500\n",
      "117/117 [==============================] - 0s 135us/step - loss: 1.4381 - acc: 0.4957\n",
      "Epoch 27/500\n",
      "117/117 [==============================] - 0s 103us/step - loss: 1.4328 - acc: 0.4957\n",
      "Epoch 28/500\n",
      "117/117 [==============================] - 0s 94us/step - loss: 1.4275 - acc: 0.5128\n",
      "Epoch 29/500\n",
      "117/117 [==============================] - 0s 101us/step - loss: 1.4222 - acc: 0.5128\n",
      "Epoch 30/500\n",
      "117/117 [==============================] - 0s 109us/step - loss: 1.4170 - acc: 0.5128\n",
      "Epoch 31/500\n",
      "117/117 [==============================] - 0s 100us/step - loss: 1.4121 - acc: 0.5128\n",
      "Epoch 32/500\n",
      "117/117 [==============================] - 0s 98us/step - loss: 1.4069 - acc: 0.5214\n",
      "Epoch 33/500\n",
      "117/117 [==============================] - 0s 122us/step - loss: 1.4017 - acc: 0.5214\n",
      "Epoch 34/500\n",
      "117/117 [==============================] - 0s 102us/step - loss: 1.3970 - acc: 0.5299\n",
      "Epoch 35/500\n",
      "117/117 [==============================] - 0s 110us/step - loss: 1.3920 - acc: 0.5299\n",
      "Epoch 36/500\n",
      "117/117 [==============================] - 0s 118us/step - loss: 1.3870 - acc: 0.5299\n",
      "Epoch 37/500\n",
      "117/117 [==============================] - 0s 107us/step - loss: 1.3820 - acc: 0.5299\n",
      "Epoch 38/500\n",
      "117/117 [==============================] - 0s 97us/step - loss: 1.3773 - acc: 0.5299\n",
      "Epoch 39/500\n",
      "117/117 [==============================] - 0s 136us/step - loss: 1.3725 - acc: 0.5299\n",
      "Epoch 40/500\n",
      "117/117 [==============================] - 0s 126us/step - loss: 1.3678 - acc: 0.5299\n",
      "Epoch 41/500\n",
      "117/117 [==============================] - 0s 116us/step - loss: 1.3632 - acc: 0.5299\n",
      "Epoch 42/500\n",
      "117/117 [==============================] - 0s 99us/step - loss: 1.3586 - acc: 0.5299\n",
      "Epoch 43/500\n",
      "117/117 [==============================] - 0s 122us/step - loss: 1.3542 - acc: 0.5556\n",
      "Epoch 44/500\n",
      "117/117 [==============================] - 0s 152us/step - loss: 1.3497 - acc: 0.5556\n",
      "Epoch 45/500\n",
      "117/117 [==============================] - 0s 150us/step - loss: 1.3454 - acc: 0.5470\n",
      "Epoch 46/500\n",
      "117/117 [==============================] - 0s 143us/step - loss: 1.3414 - acc: 0.5556\n",
      "Epoch 47/500\n",
      "117/117 [==============================] - 0s 130us/step - loss: 1.3368 - acc: 0.5641\n",
      "Epoch 48/500\n",
      "117/117 [==============================] - 0s 132us/step - loss: 1.3328 - acc: 0.5641\n",
      "Epoch 49/500\n",
      "117/117 [==============================] - 0s 133us/step - loss: 1.3287 - acc: 0.5556\n",
      "Epoch 50/500\n",
      "117/117 [==============================] - 0s 136us/step - loss: 1.3246 - acc: 0.5641\n",
      "Epoch 51/500\n",
      "117/117 [==============================] - 0s 126us/step - loss: 1.3205 - acc: 0.5726\n",
      "Epoch 52/500\n",
      "117/117 [==============================] - 0s 104us/step - loss: 1.3166 - acc: 0.5726\n",
      "Epoch 53/500\n",
      "117/117 [==============================] - 0s 98us/step - loss: 1.3129 - acc: 0.5641\n",
      "Epoch 54/500\n",
      "117/117 [==============================] - 0s 95us/step - loss: 1.3088 - acc: 0.5726\n",
      "Epoch 55/500\n",
      "117/117 [==============================] - 0s 102us/step - loss: 1.3048 - acc: 0.5726\n",
      "Epoch 56/500\n",
      "117/117 [==============================] - 0s 107us/step - loss: 1.3010 - acc: 0.5726\n",
      "Epoch 57/500\n",
      "117/117 [==============================] - 0s 120us/step - loss: 1.2973 - acc: 0.5726\n",
      "Epoch 58/500\n",
      "117/117 [==============================] - 0s 120us/step - loss: 1.2934 - acc: 0.5726\n",
      "Epoch 59/500\n",
      "117/117 [==============================] - 0s 121us/step - loss: 1.2897 - acc: 0.5726\n",
      "Epoch 60/500\n",
      "117/117 [==============================] - 0s 125us/step - loss: 1.2860 - acc: 0.5726\n",
      "Epoch 61/500\n",
      "117/117 [==============================] - 0s 106us/step - loss: 1.2822 - acc: 0.5726\n",
      "Epoch 62/500\n",
      "117/117 [==============================] - 0s 127us/step - loss: 1.2787 - acc: 0.5726\n",
      "Epoch 63/500\n",
      "117/117 [==============================] - 0s 135us/step - loss: 1.2750 - acc: 0.5726\n",
      "Epoch 64/500\n",
      "117/117 [==============================] - 0s 113us/step - loss: 1.2716 - acc: 0.5726\n",
      "Epoch 65/500\n",
      "117/117 [==============================] - 0s 112us/step - loss: 1.2679 - acc: 0.5726\n",
      "Epoch 66/500\n",
      "117/117 [==============================] - 0s 107us/step - loss: 1.2651 - acc: 0.5726\n",
      "Epoch 67/500\n",
      "117/117 [==============================] - 0s 125us/step - loss: 1.2611 - acc: 0.5726\n",
      "Epoch 68/500\n",
      "117/117 [==============================] - 0s 134us/step - loss: 1.2581 - acc: 0.5726\n",
      "Epoch 69/500\n",
      "117/117 [==============================] - 0s 113us/step - loss: 1.2544 - acc: 0.5726\n",
      "Epoch 70/500\n",
      "117/117 [==============================] - 0s 106us/step - loss: 1.2512 - acc: 0.5726\n",
      "Epoch 71/500\n",
      "117/117 [==============================] - 0s 92us/step - loss: 1.2478 - acc: 0.5726\n",
      "Epoch 72/500\n",
      "117/117 [==============================] - 0s 163us/step - loss: 1.2446 - acc: 0.5726\n",
      "Epoch 73/500\n",
      "117/117 [==============================] - 0s 138us/step - loss: 1.2413 - acc: 0.5726\n",
      "Epoch 74/500\n",
      "117/117 [==============================] - 0s 115us/step - loss: 1.2383 - acc: 0.5726\n",
      "Epoch 75/500\n",
      "117/117 [==============================] - 0s 101us/step - loss: 1.2352 - acc: 0.5726\n",
      "Epoch 76/500\n",
      "117/117 [==============================] - 0s 115us/step - loss: 1.2319 - acc: 0.5726\n",
      "Epoch 77/500\n",
      "117/117 [==============================] - 0s 135us/step - loss: 1.2291 - acc: 0.5726\n",
      "Epoch 78/500\n",
      "117/117 [==============================] - 0s 141us/step - loss: 1.2259 - acc: 0.5726\n",
      "Epoch 79/500\n",
      "117/117 [==============================] - 0s 123us/step - loss: 1.2228 - acc: 0.5726\n",
      "Epoch 80/500\n",
      "117/117 [==============================] - 0s 129us/step - loss: 1.2202 - acc: 0.5726\n",
      "Epoch 81/500\n",
      "117/117 [==============================] - 0s 132us/step - loss: 1.2169 - acc: 0.5726\n",
      "Epoch 82/500\n",
      "117/117 [==============================] - 0s 147us/step - loss: 1.2140 - acc: 0.5641\n",
      "Epoch 83/500\n",
      "117/117 [==============================] - 0s 140us/step - loss: 1.2109 - acc: 0.5726\n",
      "Epoch 84/500\n",
      "117/117 [==============================] - 0s 168us/step - loss: 1.2082 - acc: 0.5726\n",
      "Epoch 85/500\n",
      "117/117 [==============================] - 0s 136us/step - loss: 1.2052 - acc: 0.5726\n",
      "Epoch 86/500\n",
      "117/117 [==============================] - 0s 136us/step - loss: 1.2026 - acc: 0.5726\n",
      "Epoch 87/500\n",
      "117/117 [==============================] - 0s 146us/step - loss: 1.1996 - acc: 0.5726\n",
      "Epoch 88/500\n",
      "117/117 [==============================] - 0s 191us/step - loss: 1.1968 - acc: 0.5726\n",
      "Epoch 89/500\n",
      "117/117 [==============================] - 0s 141us/step - loss: 1.1941 - acc: 0.5726\n",
      "Epoch 90/500\n",
      "117/117 [==============================] - 0s 136us/step - loss: 1.1914 - acc: 0.5726\n",
      "Epoch 91/500\n",
      "117/117 [==============================] - 0s 120us/step - loss: 1.1887 - acc: 0.5726\n",
      "Epoch 92/500\n",
      "117/117 [==============================] - 0s 97us/step - loss: 1.1862 - acc: 0.5726\n",
      "Epoch 93/500\n",
      "117/117 [==============================] - 0s 105us/step - loss: 1.1833 - acc: 0.5726\n",
      "Epoch 94/500\n",
      "117/117 [==============================] - 0s 82us/step - loss: 1.1810 - acc: 0.5812\n",
      "Epoch 95/500\n",
      "117/117 [==============================] - 0s 103us/step - loss: 1.1782 - acc: 0.5812\n",
      "Epoch 96/500\n",
      "117/117 [==============================] - 0s 158us/step - loss: 1.1755 - acc: 0.5897\n",
      "Epoch 97/500\n",
      "117/117 [==============================] - 0s 118us/step - loss: 1.1733 - acc: 0.5812\n",
      "Epoch 98/500\n",
      "117/117 [==============================] - 0s 127us/step - loss: 1.1705 - acc: 0.5983\n",
      "Epoch 99/500\n",
      "117/117 [==============================] - 0s 150us/step - loss: 1.1680 - acc: 0.5983\n",
      "Epoch 100/500\n",
      "117/117 [==============================] - 0s 129us/step - loss: 1.1656 - acc: 0.5983\n",
      "Epoch 101/500\n",
      "117/117 [==============================] - 0s 121us/step - loss: 1.1632 - acc: 0.5983\n",
      "Epoch 102/500\n",
      "117/117 [==============================] - 0s 124us/step - loss: 1.1608 - acc: 0.5983\n",
      "Epoch 103/500\n",
      "117/117 [==============================] - 0s 126us/step - loss: 1.1583 - acc: 0.5983\n",
      "Epoch 104/500\n",
      "117/117 [==============================] - 0s 117us/step - loss: 1.1560 - acc: 0.5983\n",
      "Epoch 105/500\n",
      "117/117 [==============================] - 0s 131us/step - loss: 1.1535 - acc: 0.5983\n",
      "Epoch 106/500\n",
      "117/117 [==============================] - 0s 133us/step - loss: 1.1511 - acc: 0.5983\n",
      "Epoch 107/500\n",
      "117/117 [==============================] - 0s 167us/step - loss: 1.1489 - acc: 0.5983\n",
      "Epoch 108/500\n",
      "117/117 [==============================] - 0s 118us/step - loss: 1.1468 - acc: 0.5983\n",
      "Epoch 109/500\n",
      "117/117 [==============================] - 0s 88us/step - loss: 1.1444 - acc: 0.5983\n",
      "Epoch 110/500\n",
      "117/117 [==============================] - 0s 219us/step - loss: 1.1420 - acc: 0.6068\n",
      "Epoch 111/500\n",
      "117/117 [==============================] - 0s 137us/step - loss: 1.1398 - acc: 0.6068\n",
      "Epoch 112/500\n",
      "117/117 [==============================] - 0s 126us/step - loss: 1.1379 - acc: 0.6068\n",
      "Epoch 113/500\n",
      "117/117 [==============================] - 0s 119us/step - loss: 1.1353 - acc: 0.6068\n",
      "Epoch 114/500\n",
      "117/117 [==============================] - 0s 115us/step - loss: 1.1333 - acc: 0.6068\n",
      "Epoch 115/500\n",
      "117/117 [==============================] - 0s 111us/step - loss: 1.1310 - acc: 0.6068\n",
      "Epoch 116/500\n",
      "117/117 [==============================] - 0s 122us/step - loss: 1.1292 - acc: 0.6068\n",
      "Epoch 117/500\n",
      "117/117 [==============================] - 0s 113us/step - loss: 1.1270 - acc: 0.6068\n",
      "Epoch 118/500\n",
      "117/117 [==============================] - 0s 94us/step - loss: 1.1251 - acc: 0.6068\n",
      "Epoch 119/500\n",
      "117/117 [==============================] - 0s 107us/step - loss: 1.1226 - acc: 0.6068\n",
      "Epoch 120/500\n",
      "117/117 [==============================] - 0s 137us/step - loss: 1.1217 - acc: 0.6068\n",
      "Epoch 121/500\n",
      "117/117 [==============================] - 0s 129us/step - loss: 1.1185 - acc: 0.6068\n",
      "Epoch 122/500\n",
      "117/117 [==============================] - 0s 148us/step - loss: 1.1168 - acc: 0.6068\n",
      "Epoch 123/500\n",
      "117/117 [==============================] - 0s 110us/step - loss: 1.1145 - acc: 0.6068\n",
      "Epoch 124/500\n",
      "117/117 [==============================] - 0s 111us/step - loss: 1.1124 - acc: 0.6068\n",
      "Epoch 125/500\n",
      "117/117 [==============================] - 0s 128us/step - loss: 1.1105 - acc: 0.6068\n",
      "Epoch 126/500\n",
      "117/117 [==============================] - 0s 151us/step - loss: 1.1086 - acc: 0.5983\n",
      "Epoch 127/500\n",
      "117/117 [==============================] - 0s 123us/step - loss: 1.1066 - acc: 0.5983\n",
      "Epoch 128/500\n",
      "117/117 [==============================] - 0s 145us/step - loss: 1.1047 - acc: 0.5983\n",
      "Epoch 129/500\n",
      "117/117 [==============================] - 0s 129us/step - loss: 1.1031 - acc: 0.6068\n",
      "Epoch 130/500\n",
      "117/117 [==============================] - 0s 146us/step - loss: 1.1009 - acc: 0.5983\n",
      "Epoch 131/500\n",
      "117/117 [==============================] - 0s 133us/step - loss: 1.0990 - acc: 0.6068\n",
      "Epoch 132/500\n",
      "117/117 [==============================] - 0s 138us/step - loss: 1.0971 - acc: 0.6068\n",
      "Epoch 133/500\n",
      "117/117 [==============================] - 0s 124us/step - loss: 1.0952 - acc: 0.6068\n",
      "Epoch 134/500\n",
      "117/117 [==============================] - 0s 125us/step - loss: 1.0938 - acc: 0.6068\n",
      "Epoch 135/500\n",
      "117/117 [==============================] - 0s 147us/step - loss: 1.0916 - acc: 0.6068\n",
      "Epoch 136/500\n",
      "117/117 [==============================] - 0s 107us/step - loss: 1.0897 - acc: 0.6068\n",
      "Epoch 137/500\n",
      "117/117 [==============================] - 0s 131us/step - loss: 1.0879 - acc: 0.6068\n",
      "Epoch 138/500\n",
      "117/117 [==============================] - 0s 120us/step - loss: 1.0868 - acc: 0.6068\n",
      "Epoch 139/500\n",
      "117/117 [==============================] - 0s 158us/step - loss: 1.0845 - acc: 0.6068\n",
      "Epoch 140/500\n",
      "117/117 [==============================] - 0s 119us/step - loss: 1.0825 - acc: 0.6068\n",
      "Epoch 141/500\n",
      "117/117 [==============================] - 0s 145us/step - loss: 1.0812 - acc: 0.6068\n",
      "Epoch 142/500\n",
      "117/117 [==============================] - 0s 107us/step - loss: 1.0795 - acc: 0.6068\n",
      "Epoch 143/500\n",
      "117/117 [==============================] - 0s 113us/step - loss: 1.0773 - acc: 0.6068\n",
      "Epoch 144/500\n",
      "117/117 [==============================] - 0s 121us/step - loss: 1.0755 - acc: 0.6068\n",
      "Epoch 145/500\n",
      "117/117 [==============================] - 0s 183us/step - loss: 1.0738 - acc: 0.6068\n",
      "Epoch 146/500\n",
      "117/117 [==============================] - 0s 187us/step - loss: 1.0723 - acc: 0.6068\n",
      "Epoch 147/500\n",
      "117/117 [==============================] - 0s 131us/step - loss: 1.0704 - acc: 0.5983\n",
      "Epoch 148/500\n",
      "117/117 [==============================] - 0s 149us/step - loss: 1.0689 - acc: 0.5983\n",
      "Epoch 149/500\n",
      "117/117 [==============================] - 0s 117us/step - loss: 1.0671 - acc: 0.5983\n",
      "Epoch 150/500\n",
      "117/117 [==============================] - 0s 152us/step - loss: 1.0657 - acc: 0.5983\n",
      "Epoch 151/500\n",
      "117/117 [==============================] - 0s 125us/step - loss: 1.0644 - acc: 0.5983\n",
      "Epoch 152/500\n",
      "117/117 [==============================] - 0s 122us/step - loss: 1.0623 - acc: 0.5983\n",
      "Epoch 153/500\n",
      "117/117 [==============================] - 0s 132us/step - loss: 1.0606 - acc: 0.5983\n",
      "Epoch 154/500\n",
      "117/117 [==============================] - 0s 130us/step - loss: 1.0590 - acc: 0.5983\n",
      "Epoch 155/500\n",
      "117/117 [==============================] - 0s 107us/step - loss: 1.0575 - acc: 0.5983\n",
      "Epoch 156/500\n",
      "117/117 [==============================] - 0s 138us/step - loss: 1.0562 - acc: 0.5983\n",
      "Epoch 157/500\n",
      "117/117 [==============================] - 0s 117us/step - loss: 1.0546 - acc: 0.5983\n",
      "Epoch 158/500\n",
      "117/117 [==============================] - 0s 97us/step - loss: 1.0530 - acc: 0.5983\n",
      "Epoch 159/500\n",
      "117/117 [==============================] - 0s 106us/step - loss: 1.0512 - acc: 0.5983\n",
      "Epoch 160/500\n",
      "117/117 [==============================] - 0s 109us/step - loss: 1.0497 - acc: 0.5983\n",
      "Epoch 161/500\n",
      "117/117 [==============================] - 0s 153us/step - loss: 1.0481 - acc: 0.5983\n",
      "Epoch 162/500\n",
      "117/117 [==============================] - 0s 140us/step - loss: 1.0467 - acc: 0.5983\n",
      "Epoch 163/500\n",
      "117/117 [==============================] - 0s 121us/step - loss: 1.0455 - acc: 0.5983\n",
      "Epoch 164/500\n",
      "117/117 [==============================] - 0s 128us/step - loss: 1.0439 - acc: 0.5983\n",
      "Epoch 165/500\n",
      "117/117 [==============================] - 0s 99us/step - loss: 1.0425 - acc: 0.5983\n",
      "Epoch 166/500\n",
      "117/117 [==============================] - 0s 148us/step - loss: 1.0407 - acc: 0.5983\n",
      "Epoch 167/500\n",
      "117/117 [==============================] - 0s 103us/step - loss: 1.0397 - acc: 0.5983\n",
      "Epoch 168/500\n",
      "117/117 [==============================] - 0s 128us/step - loss: 1.0381 - acc: 0.5983\n",
      "Epoch 169/500\n",
      "117/117 [==============================] - 0s 110us/step - loss: 1.0367 - acc: 0.5983\n",
      "Epoch 170/500\n",
      "117/117 [==============================] - 0s 114us/step - loss: 1.0350 - acc: 0.5983\n",
      "Epoch 171/500\n",
      "117/117 [==============================] - 0s 100us/step - loss: 1.0344 - acc: 0.5983\n",
      "Epoch 172/500\n",
      "117/117 [==============================] - 0s 104us/step - loss: 1.0327 - acc: 0.6068\n",
      "Epoch 173/500\n",
      "117/117 [==============================] - 0s 102us/step - loss: 1.0310 - acc: 0.6068\n",
      "Epoch 174/500\n",
      "117/117 [==============================] - 0s 96us/step - loss: 1.0296 - acc: 0.6068\n",
      "Epoch 175/500\n",
      "117/117 [==============================] - 0s 120us/step - loss: 1.0283 - acc: 0.6068\n",
      "Epoch 176/500\n",
      "117/117 [==============================] - 0s 119us/step - loss: 1.0269 - acc: 0.6068\n",
      "Epoch 177/500\n",
      "117/117 [==============================] - 0s 138us/step - loss: 1.0256 - acc: 0.6068\n",
      "Epoch 178/500\n",
      "117/117 [==============================] - 0s 111us/step - loss: 1.0242 - acc: 0.6154\n",
      "Epoch 179/500\n",
      "117/117 [==============================] - 0s 153us/step - loss: 1.0235 - acc: 0.6154\n",
      "Epoch 180/500\n",
      "117/117 [==============================] - 0s 134us/step - loss: 1.0214 - acc: 0.6154\n",
      "Epoch 181/500\n",
      "117/117 [==============================] - 0s 112us/step - loss: 1.0205 - acc: 0.6068\n",
      "Epoch 182/500\n",
      "117/117 [==============================] - 0s 88us/step - loss: 1.0189 - acc: 0.6154\n",
      "Epoch 183/500\n",
      "117/117 [==============================] - 0s 172us/step - loss: 1.0180 - acc: 0.6154\n",
      "Epoch 184/500\n",
      "117/117 [==============================] - 0s 132us/step - loss: 1.0164 - acc: 0.6154\n",
      "Epoch 185/500\n",
      "117/117 [==============================] - 0s 130us/step - loss: 1.0151 - acc: 0.6154\n",
      "Epoch 186/500\n",
      "117/117 [==============================] - 0s 152us/step - loss: 1.0138 - acc: 0.6154\n",
      "Epoch 187/500\n",
      "117/117 [==============================] - 0s 126us/step - loss: 1.0128 - acc: 0.6325\n",
      "Epoch 188/500\n",
      "117/117 [==============================] - 0s 125us/step - loss: 1.0112 - acc: 0.6239\n",
      "Epoch 189/500\n",
      "117/117 [==============================] - 0s 151us/step - loss: 1.0102 - acc: 0.6325\n",
      "Epoch 190/500\n",
      "117/117 [==============================] - 0s 122us/step - loss: 1.0089 - acc: 0.6325\n",
      "Epoch 191/500\n",
      "117/117 [==============================] - 0s 147us/step - loss: 1.0078 - acc: 0.6154\n",
      "Epoch 192/500\n",
      "117/117 [==============================] - 0s 130us/step - loss: 1.0068 - acc: 0.6325\n",
      "Epoch 193/500\n",
      "117/117 [==============================] - 0s 157us/step - loss: 1.0053 - acc: 0.6239\n",
      "Epoch 194/500\n",
      "117/117 [==============================] - 0s 113us/step - loss: 1.0041 - acc: 0.6239\n",
      "Epoch 195/500\n",
      "117/117 [==============================] - 0s 140us/step - loss: 1.0031 - acc: 0.6239\n",
      "Epoch 196/500\n",
      "117/117 [==============================] - 0s 131us/step - loss: 1.0016 - acc: 0.6239\n",
      "Epoch 197/500\n",
      "117/117 [==============================] - 0s 121us/step - loss: 1.0009 - acc: 0.6239\n",
      "Epoch 198/500\n",
      "117/117 [==============================] - 0s 133us/step - loss: 0.9998 - acc: 0.6325\n",
      "Epoch 199/500\n",
      "117/117 [==============================] - 0s 113us/step - loss: 0.9981 - acc: 0.6325\n",
      "Epoch 200/500\n",
      "117/117 [==============================] - 0s 147us/step - loss: 0.9974 - acc: 0.6325\n",
      "Epoch 201/500\n",
      "117/117 [==============================] - 0s 111us/step - loss: 0.9963 - acc: 0.6325\n",
      "Epoch 202/500\n",
      "117/117 [==============================] - 0s 116us/step - loss: 0.9950 - acc: 0.6410\n",
      "Epoch 203/500\n",
      "117/117 [==============================] - 0s 148us/step - loss: 0.9939 - acc: 0.6410\n",
      "Epoch 204/500\n",
      "117/117 [==============================] - 0s 155us/step - loss: 0.9929 - acc: 0.6325\n",
      "Epoch 205/500\n",
      "117/117 [==============================] - 0s 181us/step - loss: 0.9915 - acc: 0.6410\n",
      "Epoch 206/500\n",
      "117/117 [==============================] - 0s 184us/step - loss: 0.9908 - acc: 0.6410\n",
      "Epoch 207/500\n",
      "117/117 [==============================] - 0s 133us/step - loss: 0.9894 - acc: 0.6410\n",
      "Epoch 208/500\n",
      "117/117 [==============================] - 0s 103us/step - loss: 0.9884 - acc: 0.6410\n",
      "Epoch 209/500\n",
      "117/117 [==============================] - 0s 104us/step - loss: 0.9876 - acc: 0.6410\n",
      "Epoch 210/500\n",
      "117/117 [==============================] - 0s 113us/step - loss: 0.9861 - acc: 0.6410\n",
      "Epoch 211/500\n",
      "117/117 [==============================] - 0s 120us/step - loss: 0.9852 - acc: 0.6410\n",
      "Epoch 212/500\n",
      "117/117 [==============================] - 0s 132us/step - loss: 0.9842 - acc: 0.6410\n",
      "Epoch 213/500\n",
      "117/117 [==============================] - 0s 107us/step - loss: 0.9833 - acc: 0.6410\n",
      "Epoch 214/500\n",
      "117/117 [==============================] - 0s 131us/step - loss: 0.9819 - acc: 0.6410\n",
      "Epoch 215/500\n",
      "117/117 [==============================] - 0s 152us/step - loss: 0.9809 - acc: 0.6410\n",
      "Epoch 216/500\n",
      "117/117 [==============================] - 0s 137us/step - loss: 0.9802 - acc: 0.6410\n",
      "Epoch 217/500\n",
      "117/117 [==============================] - 0s 151us/step - loss: 0.9792 - acc: 0.6410\n",
      "Epoch 218/500\n",
      "117/117 [==============================] - 0s 130us/step - loss: 0.9780 - acc: 0.6410\n",
      "Epoch 219/500\n",
      "117/117 [==============================] - 0s 129us/step - loss: 0.9769 - acc: 0.6410\n",
      "Epoch 220/500\n",
      "117/117 [==============================] - 0s 122us/step - loss: 0.9760 - acc: 0.6410\n",
      "Epoch 221/500\n",
      "117/117 [==============================] - 0s 117us/step - loss: 0.9749 - acc: 0.6410\n",
      "Epoch 222/500\n",
      "117/117 [==============================] - 0s 105us/step - loss: 0.9737 - acc: 0.6410\n",
      "Epoch 223/500\n",
      "117/117 [==============================] - 0s 109us/step - loss: 0.9729 - acc: 0.6496\n",
      "Epoch 224/500\n",
      "117/117 [==============================] - 0s 130us/step - loss: 0.9718 - acc: 0.6496\n",
      "Epoch 225/500\n",
      "117/117 [==============================] - 0s 143us/step - loss: 0.9709 - acc: 0.6496\n",
      "Epoch 226/500\n",
      "117/117 [==============================] - 0s 135us/step - loss: 0.9697 - acc: 0.6496\n",
      "Epoch 227/500\n",
      "117/117 [==============================] - 0s 125us/step - loss: 0.9690 - acc: 0.6496\n",
      "Epoch 228/500\n",
      "117/117 [==============================] - 0s 161us/step - loss: 0.9679 - acc: 0.6496\n",
      "Epoch 229/500\n",
      "117/117 [==============================] - 0s 104us/step - loss: 0.9672 - acc: 0.6496\n",
      "Epoch 230/500\n",
      "117/117 [==============================] - 0s 116us/step - loss: 0.9661 - acc: 0.6496\n",
      "Epoch 231/500\n",
      "117/117 [==============================] - 0s 106us/step - loss: 0.9651 - acc: 0.6496\n",
      "Epoch 232/500\n",
      "117/117 [==============================] - 0s 107us/step - loss: 0.9643 - acc: 0.6496\n",
      "Epoch 233/500\n",
      "117/117 [==============================] - 0s 106us/step - loss: 0.9636 - acc: 0.6496\n",
      "Epoch 234/500\n",
      "117/117 [==============================] - 0s 95us/step - loss: 0.9625 - acc: 0.6496\n",
      "Epoch 235/500\n",
      "117/117 [==============================] - 0s 99us/step - loss: 0.9613 - acc: 0.6496\n",
      "Epoch 236/500\n",
      "117/117 [==============================] - 0s 166us/step - loss: 0.9607 - acc: 0.6496\n",
      "Epoch 237/500\n",
      "117/117 [==============================] - 0s 105us/step - loss: 0.9594 - acc: 0.6496\n",
      "Epoch 238/500\n",
      "117/117 [==============================] - 0s 98us/step - loss: 0.9592 - acc: 0.6410\n",
      "Epoch 239/500\n",
      "117/117 [==============================] - 0s 119us/step - loss: 0.9579 - acc: 0.6496\n",
      "Epoch 240/500\n",
      "117/117 [==============================] - 0s 103us/step - loss: 0.9570 - acc: 0.6496\n",
      "Epoch 241/500\n",
      "117/117 [==============================] - 0s 97us/step - loss: 0.9561 - acc: 0.6496\n",
      "Epoch 242/500\n",
      "117/117 [==============================] - 0s 100us/step - loss: 0.9552 - acc: 0.6496\n",
      "Epoch 243/500\n",
      "117/117 [==============================] - 0s 103us/step - loss: 0.9543 - acc: 0.6496\n",
      "Epoch 244/500\n",
      "117/117 [==============================] - 0s 107us/step - loss: 0.9536 - acc: 0.6496\n",
      "Epoch 245/500\n",
      "117/117 [==============================] - 0s 116us/step - loss: 0.9526 - acc: 0.6496\n",
      "Epoch 246/500\n",
      "117/117 [==============================] - 0s 119us/step - loss: 0.9515 - acc: 0.6496\n",
      "Epoch 247/500\n",
      "117/117 [==============================] - 0s 106us/step - loss: 0.9507 - acc: 0.6496\n",
      "Epoch 248/500\n",
      "117/117 [==============================] - 0s 98us/step - loss: 0.9499 - acc: 0.6496\n",
      "Epoch 249/500\n",
      "117/117 [==============================] - 0s 102us/step - loss: 0.9495 - acc: 0.6410\n",
      "Epoch 250/500\n",
      "117/117 [==============================] - 0s 108us/step - loss: 0.9486 - acc: 0.6496\n",
      "Epoch 251/500\n",
      "117/117 [==============================] - 0s 112us/step - loss: 0.9475 - acc: 0.6496\n",
      "Epoch 252/500\n",
      "117/117 [==============================] - 0s 108us/step - loss: 0.9466 - acc: 0.6496\n",
      "Epoch 253/500\n",
      "117/117 [==============================] - 0s 131us/step - loss: 0.9457 - acc: 0.6496\n",
      "Epoch 254/500\n",
      "117/117 [==============================] - 0s 118us/step - loss: 0.9447 - acc: 0.6496\n",
      "Epoch 255/500\n",
      "117/117 [==============================] - 0s 97us/step - loss: 0.9440 - acc: 0.6496\n",
      "Epoch 256/500\n",
      "117/117 [==============================] - 0s 88us/step - loss: 0.9434 - acc: 0.6496\n",
      "Epoch 257/500\n",
      "117/117 [==============================] - 0s 135us/step - loss: 0.9424 - acc: 0.6496\n",
      "Epoch 258/500\n",
      "117/117 [==============================] - 0s 124us/step - loss: 0.9418 - acc: 0.6496\n",
      "Epoch 259/500\n",
      "117/117 [==============================] - 0s 118us/step - loss: 0.9407 - acc: 0.6496\n",
      "Epoch 260/500\n",
      "117/117 [==============================] - 0s 106us/step - loss: 0.9400 - acc: 0.6496\n",
      "Epoch 261/500\n",
      "117/117 [==============================] - 0s 98us/step - loss: 0.9397 - acc: 0.6496\n",
      "Epoch 262/500\n",
      "117/117 [==============================] - 0s 93us/step - loss: 0.9388 - acc: 0.6496\n",
      "Epoch 263/500\n",
      "117/117 [==============================] - 0s 120us/step - loss: 0.9379 - acc: 0.6496\n",
      "Epoch 264/500\n",
      "117/117 [==============================] - 0s 103us/step - loss: 0.9370 - acc: 0.6496\n",
      "Epoch 265/500\n",
      "117/117 [==============================] - 0s 129us/step - loss: 0.9363 - acc: 0.6581\n",
      "Epoch 266/500\n",
      "117/117 [==============================] - 0s 106us/step - loss: 0.9356 - acc: 0.6667\n",
      "Epoch 267/500\n",
      "117/117 [==============================] - 0s 109us/step - loss: 0.9347 - acc: 0.6496\n",
      "Epoch 268/500\n",
      "117/117 [==============================] - 0s 102us/step - loss: 0.9340 - acc: 0.6581\n",
      "Epoch 269/500\n",
      "117/117 [==============================] - 0s 92us/step - loss: 0.9338 - acc: 0.6581\n",
      "Epoch 270/500\n",
      "117/117 [==============================] - 0s 104us/step - loss: 0.9324 - acc: 0.6581\n",
      "Epoch 271/500\n",
      "117/117 [==============================] - 0s 106us/step - loss: 0.9316 - acc: 0.6581\n",
      "Epoch 272/500\n",
      "117/117 [==============================] - 0s 102us/step - loss: 0.9316 - acc: 0.6581\n",
      "Epoch 273/500\n",
      "117/117 [==============================] - 0s 121us/step - loss: 0.9301 - acc: 0.6667\n",
      "Epoch 274/500\n",
      "117/117 [==============================] - 0s 106us/step - loss: 0.9292 - acc: 0.6667\n",
      "Epoch 275/500\n",
      "117/117 [==============================] - 0s 98us/step - loss: 0.9287 - acc: 0.6667\n",
      "Epoch 276/500\n",
      "117/117 [==============================] - 0s 90us/step - loss: 0.9278 - acc: 0.6667\n",
      "Epoch 277/500\n",
      "117/117 [==============================] - 0s 111us/step - loss: 0.9271 - acc: 0.6667\n",
      "Epoch 278/500\n",
      "117/117 [==============================] - 0s 99us/step - loss: 0.9267 - acc: 0.6667\n",
      "Epoch 279/500\n",
      "117/117 [==============================] - 0s 100us/step - loss: 0.9257 - acc: 0.6667\n",
      "Epoch 280/500\n",
      "117/117 [==============================] - 0s 122us/step - loss: 0.9250 - acc: 0.6667\n",
      "Epoch 281/500\n",
      "117/117 [==============================] - 0s 135us/step - loss: 0.9248 - acc: 0.6667\n",
      "Epoch 282/500\n",
      "117/117 [==============================] - 0s 169us/step - loss: 0.9235 - acc: 0.6667\n",
      "Epoch 283/500\n",
      "117/117 [==============================] - 0s 139us/step - loss: 0.9232 - acc: 0.6581\n",
      "Epoch 284/500\n",
      "117/117 [==============================] - 0s 114us/step - loss: 0.9225 - acc: 0.6667\n",
      "Epoch 285/500\n",
      "117/117 [==============================] - 0s 119us/step - loss: 0.9214 - acc: 0.6667\n",
      "Epoch 286/500\n",
      "117/117 [==============================] - 0s 95us/step - loss: 0.9207 - acc: 0.6667\n",
      "Epoch 287/500\n",
      "117/117 [==============================] - 0s 114us/step - loss: 0.9199 - acc: 0.6667\n",
      "Epoch 288/500\n",
      "117/117 [==============================] - 0s 109us/step - loss: 0.9193 - acc: 0.6667\n",
      "Epoch 289/500\n",
      "117/117 [==============================] - 0s 120us/step - loss: 0.9186 - acc: 0.6667\n",
      "Epoch 290/500\n",
      "117/117 [==============================] - 0s 119us/step - loss: 0.9179 - acc: 0.6581\n",
      "Epoch 291/500\n",
      "117/117 [==============================] - 0s 119us/step - loss: 0.9172 - acc: 0.6667\n",
      "Epoch 292/500\n",
      "117/117 [==============================] - 0s 109us/step - loss: 0.9168 - acc: 0.6667\n",
      "Epoch 293/500\n",
      "117/117 [==============================] - 0s 106us/step - loss: 0.9160 - acc: 0.6667\n",
      "Epoch 294/500\n",
      "117/117 [==============================] - 0s 104us/step - loss: 0.9156 - acc: 0.6667\n",
      "Epoch 295/500\n",
      "117/117 [==============================] - 0s 104us/step - loss: 0.9155 - acc: 0.6667\n",
      "Epoch 296/500\n",
      "117/117 [==============================] - 0s 111us/step - loss: 0.9141 - acc: 0.6581\n",
      "Epoch 297/500\n",
      "117/117 [==============================] - 0s 126us/step - loss: 0.9131 - acc: 0.6667\n",
      "Epoch 298/500\n",
      "117/117 [==============================] - 0s 146us/step - loss: 0.9124 - acc: 0.6667\n",
      "Epoch 299/500\n",
      "117/117 [==============================] - 0s 136us/step - loss: 0.9116 - acc: 0.6667\n",
      "Epoch 300/500\n",
      "117/117 [==============================] - 0s 136us/step - loss: 0.9113 - acc: 0.6667\n",
      "Epoch 301/500\n",
      "117/117 [==============================] - 0s 146us/step - loss: 0.9105 - acc: 0.6667\n",
      "Epoch 302/500\n",
      "117/117 [==============================] - 0s 128us/step - loss: 0.9099 - acc: 0.6667\n",
      "Epoch 303/500\n",
      "117/117 [==============================] - 0s 123us/step - loss: 0.9091 - acc: 0.6667\n",
      "Epoch 304/500\n",
      "117/117 [==============================] - 0s 121us/step - loss: 0.9088 - acc: 0.6667\n",
      "Epoch 305/500\n",
      "117/117 [==============================] - 0s 137us/step - loss: 0.9077 - acc: 0.6667\n",
      "Epoch 306/500\n",
      "117/117 [==============================] - 0s 125us/step - loss: 0.9070 - acc: 0.6667\n",
      "Epoch 307/500\n",
      "117/117 [==============================] - 0s 126us/step - loss: 0.9063 - acc: 0.6667\n",
      "Epoch 308/500\n",
      "117/117 [==============================] - 0s 112us/step - loss: 0.9061 - acc: 0.6581\n",
      "Epoch 309/500\n",
      "117/117 [==============================] - 0s 162us/step - loss: 0.9054 - acc: 0.6667\n",
      "Epoch 310/500\n",
      "117/117 [==============================] - 0s 143us/step - loss: 0.9053 - acc: 0.6581\n",
      "Epoch 311/500\n",
      "117/117 [==============================] - 0s 140us/step - loss: 0.9037 - acc: 0.6667\n",
      "Epoch 312/500\n",
      "117/117 [==============================] - 0s 120us/step - loss: 0.9031 - acc: 0.6667\n",
      "Epoch 313/500\n",
      "117/117 [==============================] - 0s 125us/step - loss: 0.9029 - acc: 0.6667\n",
      "Epoch 314/500\n",
      "117/117 [==============================] - 0s 141us/step - loss: 0.9025 - acc: 0.6667\n",
      "Epoch 315/500\n",
      "117/117 [==============================] - 0s 125us/step - loss: 0.9015 - acc: 0.6667\n",
      "Epoch 316/500\n",
      "117/117 [==============================] - 0s 174us/step - loss: 0.9007 - acc: 0.6667\n",
      "Epoch 317/500\n",
      "117/117 [==============================] - 0s 144us/step - loss: 0.9000 - acc: 0.6667\n",
      "Epoch 318/500\n",
      "117/117 [==============================] - 0s 139us/step - loss: 0.8993 - acc: 0.6667\n",
      "Epoch 319/500\n",
      "117/117 [==============================] - 0s 136us/step - loss: 0.8986 - acc: 0.6667\n",
      "Epoch 320/500\n",
      "117/117 [==============================] - 0s 166us/step - loss: 0.8983 - acc: 0.6581\n",
      "Epoch 321/500\n",
      "117/117 [==============================] - 0s 145us/step - loss: 0.8974 - acc: 0.6667\n",
      "Epoch 322/500\n",
      "117/117 [==============================] - 0s 163us/step - loss: 0.8967 - acc: 0.6667\n",
      "Epoch 323/500\n",
      "117/117 [==============================] - 0s 134us/step - loss: 0.8962 - acc: 0.6667\n",
      "Epoch 324/500\n",
      "117/117 [==============================] - 0s 124us/step - loss: 0.8961 - acc: 0.6667\n",
      "Epoch 325/500\n",
      "117/117 [==============================] - 0s 141us/step - loss: 0.8950 - acc: 0.6496\n",
      "Epoch 326/500\n",
      "117/117 [==============================] - 0s 103us/step - loss: 0.8945 - acc: 0.6581\n",
      "Epoch 327/500\n",
      "117/117 [==============================] - 0s 144us/step - loss: 0.8941 - acc: 0.6581\n",
      "Epoch 328/500\n",
      "117/117 [==============================] - 0s 139us/step - loss: 0.8934 - acc: 0.6581\n",
      "Epoch 329/500\n",
      "117/117 [==============================] - 0s 126us/step - loss: 0.8924 - acc: 0.6581\n",
      "Epoch 330/500\n",
      "117/117 [==============================] - 0s 197us/step - loss: 0.8916 - acc: 0.6581\n",
      "Epoch 331/500\n",
      "117/117 [==============================] - 0s 116us/step - loss: 0.8915 - acc: 0.6496\n",
      "Epoch 332/500\n",
      "117/117 [==============================] - 0s 129us/step - loss: 0.8910 - acc: 0.6581\n",
      "Epoch 333/500\n",
      "117/117 [==============================] - 0s 148us/step - loss: 0.8899 - acc: 0.6496\n",
      "Epoch 334/500\n",
      "117/117 [==============================] - 0s 146us/step - loss: 0.8893 - acc: 0.6581\n",
      "Epoch 335/500\n",
      "117/117 [==============================] - 0s 136us/step - loss: 0.8887 - acc: 0.6581\n",
      "Epoch 336/500\n",
      "117/117 [==============================] - 0s 137us/step - loss: 0.8884 - acc: 0.6581\n",
      "Epoch 337/500\n",
      "117/117 [==============================] - 0s 123us/step - loss: 0.8878 - acc: 0.6581\n",
      "Epoch 338/500\n",
      "117/117 [==============================] - 0s 139us/step - loss: 0.8869 - acc: 0.6581\n",
      "Epoch 339/500\n",
      "117/117 [==============================] - 0s 155us/step - loss: 0.8867 - acc: 0.6581\n",
      "Epoch 340/500\n",
      "117/117 [==============================] - 0s 161us/step - loss: 0.8858 - acc: 0.6496\n",
      "Epoch 341/500\n",
      "117/117 [==============================] - 0s 140us/step - loss: 0.8852 - acc: 0.6581\n",
      "Epoch 342/500\n",
      "117/117 [==============================] - 0s 116us/step - loss: 0.8851 - acc: 0.6496\n",
      "Epoch 343/500\n",
      "117/117 [==============================] - 0s 165us/step - loss: 0.8840 - acc: 0.6581\n",
      "Epoch 344/500\n",
      "117/117 [==============================] - 0s 134us/step - loss: 0.8833 - acc: 0.6581\n",
      "Epoch 345/500\n",
      "117/117 [==============================] - 0s 127us/step - loss: 0.8833 - acc: 0.6581\n",
      "Epoch 346/500\n",
      "117/117 [==============================] - 0s 151us/step - loss: 0.8827 - acc: 0.6581\n",
      "Epoch 347/500\n",
      "117/117 [==============================] - 0s 110us/step - loss: 0.8821 - acc: 0.6581\n",
      "Epoch 348/500\n",
      "117/117 [==============================] - 0s 122us/step - loss: 0.8815 - acc: 0.6410\n",
      "Epoch 349/500\n",
      "117/117 [==============================] - 0s 134us/step - loss: 0.8808 - acc: 0.6496\n",
      "Epoch 350/500\n",
      "117/117 [==============================] - 0s 111us/step - loss: 0.8800 - acc: 0.6496\n",
      "Epoch 351/500\n",
      "117/117 [==============================] - 0s 98us/step - loss: 0.8797 - acc: 0.6581\n",
      "Epoch 352/500\n",
      "117/117 [==============================] - 0s 149us/step - loss: 0.8790 - acc: 0.6496\n",
      "Epoch 353/500\n",
      "117/117 [==============================] - 0s 161us/step - loss: 0.8783 - acc: 0.6496\n",
      "Epoch 354/500\n",
      "117/117 [==============================] - 0s 111us/step - loss: 0.8775 - acc: 0.6496\n",
      "Epoch 355/500\n",
      "117/117 [==============================] - 0s 104us/step - loss: 0.8770 - acc: 0.6496\n",
      "Epoch 356/500\n",
      "117/117 [==============================] - 0s 131us/step - loss: 0.8766 - acc: 0.6581\n",
      "Epoch 357/500\n",
      "117/117 [==============================] - 0s 129us/step - loss: 0.8763 - acc: 0.6410\n",
      "Epoch 358/500\n",
      "117/117 [==============================] - 0s 121us/step - loss: 0.8754 - acc: 0.6410\n",
      "Epoch 359/500\n",
      "117/117 [==============================] - 0s 100us/step - loss: 0.8752 - acc: 0.6410\n",
      "Epoch 360/500\n",
      "117/117 [==============================] - 0s 117us/step - loss: 0.8746 - acc: 0.6581\n",
      "Epoch 361/500\n",
      "117/117 [==============================] - 0s 100us/step - loss: 0.8739 - acc: 0.6496\n",
      "Epoch 362/500\n",
      "117/117 [==============================] - 0s 133us/step - loss: 0.8730 - acc: 0.6496\n",
      "Epoch 363/500\n",
      "117/117 [==============================] - 0s 133us/step - loss: 0.8738 - acc: 0.6496\n",
      "Epoch 364/500\n",
      "117/117 [==============================] - 0s 121us/step - loss: 0.8724 - acc: 0.6496\n",
      "Epoch 365/500\n",
      "117/117 [==============================] - 0s 92us/step - loss: 0.8719 - acc: 0.6496\n",
      "Epoch 366/500\n",
      "117/117 [==============================] - 0s 101us/step - loss: 0.8712 - acc: 0.6496\n",
      "Epoch 367/500\n",
      "117/117 [==============================] - 0s 136us/step - loss: 0.8707 - acc: 0.6496\n",
      "Epoch 368/500\n",
      "117/117 [==============================] - 0s 122us/step - loss: 0.8705 - acc: 0.6496\n",
      "Epoch 369/500\n",
      "117/117 [==============================] - 0s 176us/step - loss: 0.8692 - acc: 0.6410\n",
      "Epoch 370/500\n",
      "117/117 [==============================] - 0s 120us/step - loss: 0.8690 - acc: 0.6410\n",
      "Epoch 371/500\n",
      "117/117 [==============================] - 0s 113us/step - loss: 0.8683 - acc: 0.6496\n",
      "Epoch 372/500\n",
      "117/117 [==============================] - 0s 135us/step - loss: 0.8678 - acc: 0.6410\n",
      "Epoch 373/500\n",
      "117/117 [==============================] - 0s 154us/step - loss: 0.8674 - acc: 0.6496\n",
      "Epoch 374/500\n",
      "117/117 [==============================] - 0s 102us/step - loss: 0.8666 - acc: 0.6410\n",
      "Epoch 375/500\n",
      "117/117 [==============================] - 0s 121us/step - loss: 0.8667 - acc: 0.6496\n",
      "Epoch 376/500\n",
      "117/117 [==============================] - 0s 135us/step - loss: 0.8672 - acc: 0.6496\n",
      "Epoch 377/500\n",
      "117/117 [==============================] - 0s 131us/step - loss: 0.8655 - acc: 0.6410\n",
      "Epoch 378/500\n",
      "117/117 [==============================] - 0s 124us/step - loss: 0.8650 - acc: 0.6496\n",
      "Epoch 379/500\n",
      "117/117 [==============================] - 0s 129us/step - loss: 0.8640 - acc: 0.6410\n",
      "Epoch 380/500\n",
      "117/117 [==============================] - 0s 101us/step - loss: 0.8635 - acc: 0.6581\n",
      "Epoch 381/500\n",
      "117/117 [==============================] - 0s 151us/step - loss: 0.8631 - acc: 0.6581\n",
      "Epoch 382/500\n",
      "117/117 [==============================] - 0s 120us/step - loss: 0.8626 - acc: 0.6496\n",
      "Epoch 383/500\n",
      "117/117 [==============================] - 0s 128us/step - loss: 0.8619 - acc: 0.6581\n",
      "Epoch 384/500\n",
      "117/117 [==============================] - 0s 140us/step - loss: 0.8618 - acc: 0.6581\n",
      "Epoch 385/500\n",
      "117/117 [==============================] - 0s 119us/step - loss: 0.8609 - acc: 0.6581\n",
      "Epoch 386/500\n",
      "117/117 [==============================] - 0s 156us/step - loss: 0.8604 - acc: 0.6581\n",
      "Epoch 387/500\n",
      "117/117 [==============================] - 0s 138us/step - loss: 0.8601 - acc: 0.6496\n",
      "Epoch 388/500\n",
      "117/117 [==============================] - 0s 115us/step - loss: 0.8593 - acc: 0.6581\n",
      "Epoch 389/500\n",
      "117/117 [==============================] - 0s 107us/step - loss: 0.8590 - acc: 0.6581\n",
      "Epoch 390/500\n",
      "117/117 [==============================] - 0s 100us/step - loss: 0.8588 - acc: 0.6581\n",
      "Epoch 391/500\n",
      "117/117 [==============================] - 0s 113us/step - loss: 0.8583 - acc: 0.6581\n",
      "Epoch 392/500\n",
      "117/117 [==============================] - 0s 92us/step - loss: 0.8577 - acc: 0.6496\n",
      "Epoch 393/500\n",
      "117/117 [==============================] - 0s 106us/step - loss: 0.8568 - acc: 0.6581\n",
      "Epoch 394/500\n",
      "117/117 [==============================] - 0s 104us/step - loss: 0.8565 - acc: 0.6581\n",
      "Epoch 395/500\n",
      "117/117 [==============================] - 0s 122us/step - loss: 0.8559 - acc: 0.6581\n",
      "Epoch 396/500\n",
      "117/117 [==============================] - 0s 111us/step - loss: 0.8552 - acc: 0.6496\n",
      "Epoch 397/500\n",
      "117/117 [==============================] - 0s 109us/step - loss: 0.8547 - acc: 0.6496\n",
      "Epoch 398/500\n",
      "117/117 [==============================] - 0s 139us/step - loss: 0.8559 - acc: 0.6496\n",
      "Epoch 399/500\n",
      "117/117 [==============================] - 0s 116us/step - loss: 0.8538 - acc: 0.6581\n",
      "Epoch 400/500\n",
      "117/117 [==============================] - 0s 98us/step - loss: 0.8535 - acc: 0.6496\n",
      "Epoch 401/500\n",
      "117/117 [==============================] - 0s 149us/step - loss: 0.8533 - acc: 0.6496\n",
      "Epoch 402/500\n",
      "117/117 [==============================] - 0s 118us/step - loss: 0.8525 - acc: 0.6496\n",
      "Epoch 403/500\n",
      "117/117 [==============================] - 0s 97us/step - loss: 0.8520 - acc: 0.6581\n",
      "Epoch 404/500\n",
      "117/117 [==============================] - 0s 94us/step - loss: 0.8515 - acc: 0.6496\n",
      "Epoch 405/500\n",
      "117/117 [==============================] - 0s 116us/step - loss: 0.8511 - acc: 0.6496\n",
      "Epoch 406/500\n",
      "117/117 [==============================] - 0s 117us/step - loss: 0.8509 - acc: 0.6496\n",
      "Epoch 407/500\n",
      "117/117 [==============================] - 0s 119us/step - loss: 0.8500 - acc: 0.6496\n",
      "Epoch 408/500\n",
      "117/117 [==============================] - 0s 119us/step - loss: 0.8494 - acc: 0.6496\n",
      "Epoch 409/500\n",
      "117/117 [==============================] - 0s 105us/step - loss: 0.8503 - acc: 0.6496\n",
      "Epoch 410/500\n",
      "117/117 [==============================] - 0s 131us/step - loss: 0.8486 - acc: 0.6496\n",
      "Epoch 411/500\n",
      "117/117 [==============================] - 0s 125us/step - loss: 0.8483 - acc: 0.6496\n",
      "Epoch 412/500\n",
      "117/117 [==============================] - 0s 131us/step - loss: 0.8477 - acc: 0.6496\n",
      "Epoch 413/500\n",
      "117/117 [==============================] - 0s 117us/step - loss: 0.8474 - acc: 0.6496\n",
      "Epoch 414/500\n",
      "117/117 [==============================] - 0s 135us/step - loss: 0.8468 - acc: 0.6496\n",
      "Epoch 415/500\n",
      "117/117 [==============================] - 0s 135us/step - loss: 0.8468 - acc: 0.6496\n",
      "Epoch 416/500\n",
      "117/117 [==============================] - 0s 101us/step - loss: 0.8460 - acc: 0.6496\n",
      "Epoch 417/500\n",
      "117/117 [==============================] - 0s 104us/step - loss: 0.8454 - acc: 0.6496\n",
      "Epoch 418/500\n",
      "117/117 [==============================] - 0s 140us/step - loss: 0.8450 - acc: 0.6496\n",
      "Epoch 419/500\n",
      "117/117 [==============================] - 0s 111us/step - loss: 0.8446 - acc: 0.6581\n",
      "Epoch 420/500\n",
      "117/117 [==============================] - 0s 153us/step - loss: 0.8439 - acc: 0.6496\n",
      "Epoch 421/500\n",
      "117/117 [==============================] - 0s 184us/step - loss: 0.8436 - acc: 0.6496\n",
      "Epoch 422/500\n",
      "117/117 [==============================] - 0s 94us/step - loss: 0.8430 - acc: 0.6410\n",
      "Epoch 423/500\n",
      "117/117 [==============================] - 0s 118us/step - loss: 0.8424 - acc: 0.6410\n",
      "Epoch 424/500\n",
      "117/117 [==============================] - 0s 149us/step - loss: 0.8420 - acc: 0.6410\n",
      "Epoch 425/500\n",
      "117/117 [==============================] - 0s 113us/step - loss: 0.8414 - acc: 0.6410\n",
      "Epoch 426/500\n",
      "117/117 [==============================] - 0s 107us/step - loss: 0.8415 - acc: 0.6496\n",
      "Epoch 427/500\n",
      "117/117 [==============================] - 0s 105us/step - loss: 0.8406 - acc: 0.6410\n",
      "Epoch 428/500\n",
      "117/117 [==============================] - 0s 105us/step - loss: 0.8406 - acc: 0.6581\n",
      "Epoch 429/500\n",
      "117/117 [==============================] - 0s 97us/step - loss: 0.8399 - acc: 0.6496\n",
      "Epoch 430/500\n",
      "117/117 [==============================] - 0s 99us/step - loss: 0.8397 - acc: 0.6410\n",
      "Epoch 431/500\n",
      "117/117 [==============================] - 0s 97us/step - loss: 0.8386 - acc: 0.6496\n",
      "Epoch 432/500\n",
      "117/117 [==============================] - 0s 105us/step - loss: 0.8383 - acc: 0.6496\n",
      "Epoch 433/500\n",
      "117/117 [==============================] - 0s 86us/step - loss: 0.8377 - acc: 0.6496\n",
      "Epoch 434/500\n",
      "117/117 [==============================] - 0s 91us/step - loss: 0.8379 - acc: 0.6496\n",
      "Epoch 435/500\n",
      "117/117 [==============================] - 0s 109us/step - loss: 0.8370 - acc: 0.6410\n",
      "Epoch 436/500\n",
      "117/117 [==============================] - 0s 119us/step - loss: 0.8368 - acc: 0.6410\n",
      "Epoch 437/500\n",
      "117/117 [==============================] - 0s 123us/step - loss: 0.8367 - acc: 0.6410\n",
      "Epoch 438/500\n",
      "117/117 [==============================] - 0s 130us/step - loss: 0.8359 - acc: 0.6410\n",
      "Epoch 439/500\n",
      "117/117 [==============================] - 0s 145us/step - loss: 0.8353 - acc: 0.6410\n",
      "Epoch 440/500\n",
      "117/117 [==============================] - 0s 130us/step - loss: 0.8354 - acc: 0.6496\n",
      "Epoch 441/500\n",
      "117/117 [==============================] - 0s 138us/step - loss: 0.8342 - acc: 0.6496\n",
      "Epoch 442/500\n",
      "117/117 [==============================] - 0s 116us/step - loss: 0.8338 - acc: 0.6410\n",
      "Epoch 443/500\n",
      "117/117 [==============================] - 0s 156us/step - loss: 0.8333 - acc: 0.6496\n",
      "Epoch 444/500\n",
      "117/117 [==============================] - 0s 126us/step - loss: 0.8327 - acc: 0.6581\n",
      "Epoch 445/500\n",
      "117/117 [==============================] - 0s 96us/step - loss: 0.8323 - acc: 0.6581\n",
      "Epoch 446/500\n",
      "117/117 [==============================] - 0s 98us/step - loss: 0.8325 - acc: 0.6496\n",
      "Epoch 447/500\n",
      "117/117 [==============================] - 0s 110us/step - loss: 0.8316 - acc: 0.6581\n",
      "Epoch 448/500\n",
      "117/117 [==============================] - 0s 136us/step - loss: 0.8316 - acc: 0.6667\n",
      "Epoch 449/500\n",
      "117/117 [==============================] - 0s 127us/step - loss: 0.8308 - acc: 0.6496\n",
      "Epoch 450/500\n",
      "117/117 [==============================] - 0s 212us/step - loss: 0.8302 - acc: 0.6496\n",
      "Epoch 451/500\n",
      "117/117 [==============================] - 0s 105us/step - loss: 0.8300 - acc: 0.6496\n",
      "Epoch 452/500\n",
      "117/117 [==============================] - 0s 165us/step - loss: 0.8299 - acc: 0.6496\n",
      "Epoch 453/500\n",
      "117/117 [==============================] - 0s 141us/step - loss: 0.8289 - acc: 0.6581\n",
      "Epoch 454/500\n",
      "117/117 [==============================] - 0s 134us/step - loss: 0.8285 - acc: 0.6496\n",
      "Epoch 455/500\n",
      "117/117 [==============================] - 0s 113us/step - loss: 0.8281 - acc: 0.6581\n",
      "Epoch 456/500\n",
      "117/117 [==============================] - 0s 113us/step - loss: 0.8280 - acc: 0.6496\n",
      "Epoch 457/500\n",
      "117/117 [==============================] - 0s 126us/step - loss: 0.8273 - acc: 0.6496\n",
      "Epoch 458/500\n",
      "117/117 [==============================] - 0s 139us/step - loss: 0.8271 - acc: 0.6496\n",
      "Epoch 459/500\n",
      "117/117 [==============================] - 0s 114us/step - loss: 0.8263 - acc: 0.6496\n",
      "Epoch 460/500\n",
      "117/117 [==============================] - 0s 126us/step - loss: 0.8259 - acc: 0.6496\n",
      "Epoch 461/500\n",
      "117/117 [==============================] - 0s 121us/step - loss: 0.8258 - acc: 0.6581\n",
      "Epoch 462/500\n",
      "117/117 [==============================] - 0s 148us/step - loss: 0.8256 - acc: 0.6496\n",
      "Epoch 463/500\n",
      "117/117 [==============================] - 0s 107us/step - loss: 0.8257 - acc: 0.6496\n",
      "Epoch 464/500\n",
      "117/117 [==============================] - 0s 108us/step - loss: 0.8240 - acc: 0.6496\n",
      "Epoch 465/500\n",
      "117/117 [==============================] - 0s 113us/step - loss: 0.8238 - acc: 0.6496\n",
      "Epoch 466/500\n",
      "117/117 [==============================] - 0s 92us/step - loss: 0.8232 - acc: 0.6496\n",
      "Epoch 467/500\n",
      "117/117 [==============================] - 0s 120us/step - loss: 0.8226 - acc: 0.6496\n",
      "Epoch 468/500\n",
      "117/117 [==============================] - 0s 95us/step - loss: 0.8225 - acc: 0.6496\n",
      "Epoch 469/500\n",
      "117/117 [==============================] - 0s 139us/step - loss: 0.8219 - acc: 0.6496\n",
      "Epoch 470/500\n",
      "117/117 [==============================] - 0s 149us/step - loss: 0.8216 - acc: 0.6496\n",
      "Epoch 471/500\n",
      "117/117 [==============================] - 0s 114us/step - loss: 0.8214 - acc: 0.6496\n",
      "Epoch 472/500\n",
      "117/117 [==============================] - 0s 97us/step - loss: 0.8209 - acc: 0.6496\n",
      "Epoch 473/500\n",
      "117/117 [==============================] - 0s 118us/step - loss: 0.8203 - acc: 0.6496\n",
      "Epoch 474/500\n",
      "117/117 [==============================] - 0s 123us/step - loss: 0.8197 - acc: 0.6496\n",
      "Epoch 475/500\n",
      "117/117 [==============================] - 0s 160us/step - loss: 0.8198 - acc: 0.6496\n",
      "Epoch 476/500\n",
      "117/117 [==============================] - 0s 113us/step - loss: 0.8193 - acc: 0.6581\n",
      "Epoch 477/500\n",
      "117/117 [==============================] - 0s 135us/step - loss: 0.8188 - acc: 0.6581\n",
      "Epoch 478/500\n",
      "117/117 [==============================] - 0s 129us/step - loss: 0.8180 - acc: 0.6496\n",
      "Epoch 479/500\n",
      "117/117 [==============================] - 0s 124us/step - loss: 0.8176 - acc: 0.6496\n",
      "Epoch 480/500\n",
      "117/117 [==============================] - 0s 123us/step - loss: 0.8171 - acc: 0.6496\n",
      "Epoch 481/500\n",
      "117/117 [==============================] - 0s 110us/step - loss: 0.8173 - acc: 0.6496\n",
      "Epoch 482/500\n",
      "117/117 [==============================] - 0s 136us/step - loss: 0.8173 - acc: 0.6496\n",
      "Epoch 483/500\n",
      "117/117 [==============================] - 0s 110us/step - loss: 0.8160 - acc: 0.6410\n",
      "Epoch 484/500\n",
      "117/117 [==============================] - 0s 109us/step - loss: 0.8156 - acc: 0.6496\n",
      "Epoch 485/500\n",
      "117/117 [==============================] - 0s 141us/step - loss: 0.8154 - acc: 0.6496\n",
      "Epoch 486/500\n",
      "117/117 [==============================] - 0s 106us/step - loss: 0.8150 - acc: 0.6496\n",
      "Epoch 487/500\n",
      "117/117 [==============================] - 0s 115us/step - loss: 0.8148 - acc: 0.6496\n",
      "Epoch 488/500\n",
      "117/117 [==============================] - 0s 91us/step - loss: 0.8145 - acc: 0.6410\n",
      "Epoch 489/500\n",
      "117/117 [==============================] - 0s 110us/step - loss: 0.8135 - acc: 0.6496\n",
      "Epoch 490/500\n",
      "117/117 [==============================] - 0s 104us/step - loss: 0.8131 - acc: 0.6496\n",
      "Epoch 491/500\n",
      "117/117 [==============================] - 0s 106us/step - loss: 0.8127 - acc: 0.6410\n",
      "Epoch 492/500\n",
      "117/117 [==============================] - 0s 145us/step - loss: 0.8121 - acc: 0.6496\n",
      "Epoch 493/500\n",
      "117/117 [==============================] - 0s 101us/step - loss: 0.8121 - acc: 0.6496\n",
      "Epoch 494/500\n",
      "117/117 [==============================] - 0s 151us/step - loss: 0.8114 - acc: 0.6496\n",
      "Epoch 495/500\n",
      "117/117 [==============================] - 0s 124us/step - loss: 0.8109 - acc: 0.6410\n",
      "Epoch 496/500\n",
      "117/117 [==============================] - 0s 156us/step - loss: 0.8108 - acc: 0.6496\n",
      "Epoch 497/500\n",
      "117/117 [==============================] - 0s 113us/step - loss: 0.8109 - acc: 0.6581\n",
      "Epoch 498/500\n",
      "117/117 [==============================] - 0s 115us/step - loss: 0.8100 - acc: 0.6410\n",
      "Epoch 499/500\n",
      "117/117 [==============================] - 0s 182us/step - loss: 0.8096 - acc: 0.6496\n",
      "Epoch 500/500\n",
      "117/117 [==============================] - 0s 162us/step - loss: 0.8090 - acc: 0.6496\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd890568f60>"
      ]
     },
     "execution_count": 158,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "import keras\n",
    "from keras.optimizers import SGD\n",
    "model = Sequential()\n",
    "model.add(Dense(150,input_dim=11, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(5, activation='softmax'))   \n",
    "                                            \n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "# Fitting the NN to the Training set\n",
    "model.fit(input_preprocessor.transform(X_train), pd.get_dummies(y_train), \n",
    "               batch_size = 60, \n",
    "               epochs = 500, validation_split=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 154
    },
    "colab_type": "code",
    "id": "TwKS0MtaIo7X",
    "outputId": "0dc61ea2-c9f2-4772-ba9a-48d15d9e2c36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 1 1 4 0 2 2 3 1 1 4 0 4 0 3 4 3 1 2 4 2 2 0 3 2 4 2 1 4 0 4 1 3 1 1 4 0\n",
      " 3 1]\n",
      "['Very Low', 'High', 'High', 'Very Low', 'Average', 'Low', 'Low', 'Very High', 'High', 'High', 'Very Low', 'Average', 'Very Low', 'Average', 'Very High', 'Very Low', 'Very High', 'High', 'Low', 'Very Low', 'Low', 'Low', 'Average', 'Very High', 'Low', 'Very Low', 'Low', 'High', 'Very Low', 'Average', 'Very Low', 'High', 'Very High', 'High', 'High', 'Very Low', 'Average', 'Very High', 'High']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>mse</th>\n",
       "      <th>rmse</th>\n",
       "      <th>mae</th>\n",
       "      <th>r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.620742</td>\n",
       "      <td>0.65619</td>\n",
       "      <td>0.643968</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  f1_score  precision    recall  mse  rmse  mae  r2\n",
       "0  0.615385  0.620742    0.65619  0.643968    0     0    0   0"
      ]
     },
     "execution_count": 159,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using predict_classes() for multi-class data to return predicted class index.\n",
    "\n",
    "print(model.predict_classes(input_preprocessor.transform(X_test)))\n",
    "\n",
    "prediction_index=model.predict_classes(input_preprocessor.transform(X_test))\n",
    "\n",
    "#Now lets run some code to get keras to return the label rather than the index...\n",
    "\n",
    "# get labels from one hot encoded y_train data\n",
    "labels=pd.get_dummies(y_train).columns\n",
    "\n",
    "# Function to use to return label from column index location\n",
    "def index_to_label(labels,index_n): \n",
    "    return labels[index_n]\n",
    "    \n",
    "# Example: return label at predicted index location 1\n",
    "index_to_label(labels,1)\n",
    "\n",
    "# Iterate through all predicted indices using map method\n",
    "\n",
    "predicted_labels=list(map(lambda x: labels[x], prediction_index))\n",
    "print(predicted_labels)\n",
    "# Now we can extract some evaluative metrics to use for model submission\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import pandas as pd\n",
    "from math import sqrt\n",
    "\n",
    "def model_eval_metrics(y_true, y_pred,classification=\"TRUE\"):\n",
    "     if classification==\"TRUE\":\n",
    "        accuracy_eval = accuracy_score(y_true, y_pred)\n",
    "        f1_score_eval = f1_score(y_true, y_pred,average=\"macro\",zero_division=0)\n",
    "        precision_eval = precision_score(y_true, y_pred,average=\"macro\",zero_division=0)\n",
    "        recall_eval = recall_score(y_true, y_pred,average=\"macro\",zero_division=0)\n",
    "        mse_eval = 0\n",
    "        rmse_eval = 0\n",
    "        mae_eval = 0\n",
    "        r2_eval = 0\n",
    "        metricdata = {'accuracy': [accuracy_eval], 'f1_score': [f1_score_eval], 'precision': [precision_eval], 'recall': [recall_eval], 'mse': [mse_eval], 'rmse': [rmse_eval], 'mae': [mae_eval], 'r2': [r2_eval]}\n",
    "        finalmetricdata = pd.DataFrame.from_dict(metricdata)\n",
    "     else:\n",
    "        accuracy_eval = 0\n",
    "        f1_score_eval = 0\n",
    "        precision_eval = 0\n",
    "        recall_eval = 0\n",
    "        mse_eval = mean_squared_error(y_true, y_pred)\n",
    "        rmse_eval = sqrt(mean_squared_error(y_true, y_pred))\n",
    "        mae_eval = mean_absolute_error(y_true, y_pred)\n",
    "        r2_eval = r2_score(y_true, y_pred)\n",
    "        metricdata = {'accuracy': [accuracy_eval], 'f1_score': [f1_score_eval], 'precision': [precision_eval], 'recall': [recall_eval], 'mse': [mse_eval], 'rmse': [rmse_eval], 'mae': [mae_eval], 'r2': [r2_eval]}\n",
    "        finalmetricdata = pd.DataFrame.from_dict(metricdata)\n",
    "     return finalmetricdata\n",
    "\n",
    "model_eval_metrics( y_test,predicted_labels,classification=\"TRUE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "nIqWtWzSctdt",
    "outputId": "0742fca8-80aa-4483-cc70-e10238f65932"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>mse</th>\n",
       "      <th>rmse</th>\n",
       "      <th>mae</th>\n",
       "      <th>r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.620742</td>\n",
       "      <td>0.65619</td>\n",
       "      <td>0.643968</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  f1_score  precision    recall  mse  rmse  mae  r2\n",
       "0  0.615385  0.620742    0.65619  0.643968    0     0    0   0"
      ]
     },
     "execution_count": 160,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add metrics to submittable object\n",
    "modelevalobject=model_eval_metrics(y_test,predicted_labels,classification=\"TRUE\")\n",
    "\n",
    "modelevalobject\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IkG8mCVwvUyP"
   },
   "source": [
    "### The parameters are as follows: hidden layer is 3. Nuerons for first layer is 150. Nuerons for second layer is 100. Batch_size is 60. epochs is 500. validation_split is 0\n",
    "### And the model's prediction accuracy rate is around 0.62, the f1_score is also around 0.62, the precision is 0.656 and the recall is 0.644."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0i420WmtwUU7"
   },
   "source": [
    "# Submit best model (Model 3) to the leader board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 667
    },
    "colab_type": "code",
    "id": "9aG6HW3otvdC",
    "outputId": "679cbfca-17cf-4f7b-c44d-b0eb45140495"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras2onnx\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/df/38475abd5ef1e0c5a19f021add159e8a07d10525b2c01f13bf06371aedd4/keras2onnx-1.6.0-py3-none-any.whl (219kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 225kB 2.9MB/s \n",
      "\u001b[?25hCollecting fire\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/69/faeaae8687f4de0f5973694d02e9d6c3eb827636a009157352d98de1129e/fire-0.2.1.tar.gz (76kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 81kB 7.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from keras2onnx) (3.10.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras2onnx) (1.17.5)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from keras2onnx) (2.21.0)\n",
      "Collecting onnxconverter-common>=1.6.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/77/3d/6112c19223d1eabbedf1b063567034e1463a11d7c82d1820f26b75d14e3c/onnxconverter_common-1.6.0-py2.py3-none-any.whl (43kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51kB 5.8MB/s \n",
      "\u001b[?25hCollecting onnx\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/f4/e126b60d109ad1e80020071484b935980b7cce1e4796073aab086a2d6902/onnx-1.6.0-cp36-cp36m-manylinux1_x86_64.whl (4.8MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.8MB 8.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from fire->keras2onnx) (1.12.0)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from fire->keras2onnx) (1.1.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->keras2onnx) (45.1.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->keras2onnx) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->keras2onnx) (2019.11.28)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->keras2onnx) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->keras2onnx) (1.24.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.6/dist-packages (from onnx->keras2onnx) (3.6.6)\n",
      "Building wheels for collected packages: fire\n",
      "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for fire: filename=fire-0.2.1-py2.py3-none-any.whl size=103528 sha256=f058ea543e3b342726d37cafc216031b334fcf12c040e1efc6b7b8e709171f32\n",
      "  Stored in directory: /root/.cache/pip/wheels/31/9c/c0/07b6dc7faf1844bb4688f46b569efe6cafaa2179c95db821da\n",
      "Successfully built fire\n",
      "Installing collected packages: fire, onnx, onnxconverter-common, keras2onnx\n",
      "Successfully installed fire-0.2.1 keras2onnx-1.6.0 onnx-1.6.0 onnxconverter-common-1.6.0\n",
      "Collecting onnxruntime\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/13/1ac0397bcffb30d03af3de80eae9b960883d2e43e3a562423260948390f5/onnxruntime-1.1.1-cp36-cp36m-manylinux1_x86_64.whl (3.6MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.6MB 2.9MB/s \n",
      "\u001b[?25hInstalling collected packages: onnxruntime\n",
      "Successfully installed onnxruntime-1.1.1\n"
     ]
    }
   ],
   "source": [
    "# Load libraries for onnx model conversion (keras to onnx)\n",
    "! pip3 install keras2onnx\n",
    "! pip3 install onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NUTGs3Yxt26y"
   },
   "outputs": [],
   "source": [
    "#Convert keras model object to onnx and then save it to .onnx file\n",
    "import os\n",
    "\n",
    "if not os.path.exists('mymodel.onnx'):\n",
    "    from keras2onnx import convert_keras\n",
    "    onx = convert_keras(model, 'mymodel.onnx')\n",
    "    with open(\"mymodel.onnx\", \"wb\") as f:\n",
    "        f.write(onx.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181
    },
    "colab_type": "code",
    "id": "8oi5NRMyuHh7",
    "outputId": "44c7afa6-6b68-4413-d2be-77e59feb3ce3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://github.com/mikedparrott/aimodelshare/blob/master/aimodelshare-0.0.2.tar.gz?raw=true\n",
      "  Using cached https://github.com/mikedparrott/aimodelshare/blob/master/aimodelshare-0.0.2.tar.gz?raw=true\n",
      "Requirement already satisfied (use --upgrade to upgrade): aimodelshare==0.0.2 from https://github.com/mikedparrott/aimodelshare/blob/master/aimodelshare-0.0.2.tar.gz?raw=true in /usr/local/lib/python3.6/dist-packages\n",
      "Building wheels for collected packages: aimodelshare\n",
      "  Building wheel for aimodelshare (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for aimodelshare: filename=aimodelshare-0.0.2-cp36-none-any.whl size=5375 sha256=7033adbf8f460886cbfd617b1d6464dbed9d546f4ef47651d014a471f676d235\n",
      "  Stored in directory: /root/.cache/pip/wheels/31/8d/ac/09cb6ef7374ec79e02843c347195e5478144006b11def6799a\n",
      "Successfully built aimodelshare\n"
     ]
    }
   ],
   "source": [
    "#install aimodelshare library\n",
    "! pip3 install https://github.com/mikedparrott/aimodelshare/blob/master/aimodelshare-0.0.2.tar.gz?raw=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1tjfuQZtuLfK"
   },
   "outputs": [],
   "source": [
    "# Loading AWS keys necessary to submit model.  Loading to object, so we don't print them out in our notebook\n",
    "\n",
    "aws_key_password_region = pickle.load( open( \"worldhappiness_modelsubmission_keys.pkl\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zzj9bksFuuXL"
   },
   "outputs": [],
   "source": [
    "# Example Model Pre-launched into Model Share Site\n",
    "apiurl=\"https://btuvanmi55.execute-api.us-east-1.amazonaws.com/prod/m\"\n",
    "username = \"XU\"\n",
    "password = \"MayDay12345\"\n",
    "\n",
    "region='us-east-1'\n",
    "model_filepath=\"mymodel.onnx\"   \n",
    "preprocessor_filepath=\"preprocessor.pkl\"\n",
    "preprocessor=\"TRUE\"\n",
    "\n",
    "trainingdata=X_train\n",
    "\n",
    "# Set aws keys for this project (these keys give you access to collaborate on a single project)\n",
    "\n",
    "#Importing from object that stores keys so we do not print out keys for others to see.\n",
    "\n",
    "aws_key_password_region = pickle.load( open( \"worldhappiness_modelsubmission_keys.pkl\", \"rb\" ) )\n",
    "\n",
    "aws_key=aws_key_password_region[0]\n",
    "aws_password=aws_key_password_region[1]\n",
    "region=aws_key_password_region[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "_d1yifYju03q",
    "outputId": "b329793c-a473-4245-884c-b26c40b01394"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"mymodel.onnx\" has been loaded to version 94 of your prediction API.\n",
      "This version of the model will be used by your prediction api for all future predictions automatically.\n",
      "If you wish to use an older version of the model, please reference the getting started guide at aimodelshare.com.\n"
     ]
    }
   ],
   "source": [
    "# Submit your model using submit_model() function\n",
    "# Works with models and preprocessors. \n",
    "import aimodelshare as ai\n",
    "\n",
    "ai.submit_model(model_filepath=model_filepath, model_eval_metrics=modelevalobject,apiurl=apiurl, username=username, password=password, aws_key=aws_key,aws_password=aws_password, region=region, trainingdata=trainingdata,preprocessor_filepath=preprocessor_filepath,preprocessor=preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l0kKfc0mu52c"
   },
   "outputs": [],
   "source": [
    "# arguments required to get leaderboard below\n",
    "apiurl=\"https://btuvanmi55.execute-api.us-east-1.amazonaws.com/prod/m\"\n",
    "username = \"XU\"\n",
    "password = \"MayDay12345\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 474
    },
    "colab_type": "code",
    "id": "wz6Z7GJlu-UE",
    "outputId": "b611f714-6a34-4c51-e988-b92fc4975b83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEADERBOARD RANKINGS:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>mse</th>\n",
       "      <th>rmse</th>\n",
       "      <th>mae</th>\n",
       "      <th>r2</th>\n",
       "      <th>username</th>\n",
       "      <th>model_version</th>\n",
       "      <th>avg_ranking_classification</th>\n",
       "      <th>avg_ranking_regression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.717949</td>\n",
       "      <td>0.717857</td>\n",
       "      <td>0.717857</td>\n",
       "      <td>0.727778</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3scman</td>\n",
       "      <td>85</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.717949</td>\n",
       "      <td>0.713796</td>\n",
       "      <td>0.719444</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3scman</td>\n",
       "      <td>70</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.675975</td>\n",
       "      <td>0.754286</td>\n",
       "      <td>0.700952</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dhoward97</td>\n",
       "      <td>69</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.693333</td>\n",
       "      <td>0.700397</td>\n",
       "      <td>0.702778</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3scman</td>\n",
       "      <td>62</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.641026</td>\n",
       "      <td>0.642381</td>\n",
       "      <td>0.743590</td>\n",
       "      <td>0.682273</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>SUN-Wenjun</td>\n",
       "      <td>83</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.343861</td>\n",
       "      <td>0.348889</td>\n",
       "      <td>0.422222</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>zivzach</td>\n",
       "      <td>50</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.303896</td>\n",
       "      <td>0.340260</td>\n",
       "      <td>0.425000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>abhay_07</td>\n",
       "      <td>18</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.337698</td>\n",
       "      <td>0.385556</td>\n",
       "      <td>0.322222</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>username2</td>\n",
       "      <td>6</td>\n",
       "      <td>34.333333</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.337698</td>\n",
       "      <td>0.385556</td>\n",
       "      <td>0.322222</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>username2</td>\n",
       "      <td>3</td>\n",
       "      <td>34.333333</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.337698</td>\n",
       "      <td>0.385556</td>\n",
       "      <td>0.322222</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>username2</td>\n",
       "      <td>2</td>\n",
       "      <td>34.333333</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>94 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    accuracy  f1_score  ...  avg_ranking_classification  avg_ranking_regression\n",
       "25  0.717949  0.717857  ...                    2.333333                     1.0\n",
       "15  0.717949  0.713796  ...                    2.333333                     1.0\n",
       "7   0.666667  0.675975  ...                    2.666667                     1.0\n",
       "58  0.692308  0.693333  ...                    4.000000                     1.0\n",
       "62  0.641026  0.642381  ...                    4.000000                     1.0\n",
       "..       ...       ...  ...                         ...                     ...\n",
       "42  0.384615  0.343861  ...                   34.000000                     1.0\n",
       "65  0.384615  0.303896  ...                   35.000000                     1.0\n",
       "44  0.333333  0.337698  ...                   34.333333                     1.0\n",
       "77  0.333333  0.337698  ...                   34.333333                     1.0\n",
       "69  0.333333  0.337698  ...                   34.333333                     1.0\n",
       "\n",
       "[94 rows x 12 columns]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import aimodelshare as ai\n",
    "\n",
    "leaderboard = ai.get_leaderboard(apiurl, username, password, aws_key, aws_password, region)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "AML_HW1_YizhenXu.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
